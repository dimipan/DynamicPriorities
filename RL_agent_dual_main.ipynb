{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment_sar import SARrobotEnv\n",
    "from agents import QLearningAgentFlat, QLearningAgentMaxInfoRL_ADVANCED\n",
    "from robot_utils import RunningParameters, agent_config\n",
    "from evaluation import evaluate_trained_policy\n",
    "import time\n",
    "param = RunningParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from environment_sar import SARrobotEnv\n",
    "from agents import QLearningAgentFlat, QLearningAgentMaxInfoRL_ADVANCED\n",
    "\n",
    "# Configuration\n",
    "GRID_ROWS = 4\n",
    "GRID_COLS = 4\n",
    "INFO_POINTS = 3  # Number of information points to collect\n",
    "NUM_EPISODES = 5000\n",
    "LOG_DIR = \"./logs/comparison_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "POLICY_DIR = os.path.join(LOG_DIR, \"policies\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "os.makedirs(POLICY_DIR, exist_ok=True)\n",
    "\n",
    "# RL parameters\n",
    "ALPHA = 0.1\n",
    "GAMMA = 0.99\n",
    "EPSILON_MAX = 1.0\n",
    "DECAY_RATE = 2\n",
    "EPSILON_MIN = 0.05\n",
    "\n",
    "# Define experiments\n",
    "experiments = [\n",
    "    {\n",
    "        \"name\": \"Flat_Static\",\n",
    "        \"agent_class\": QLearningAgentFlat,\n",
    "        \"sparse_reward\": False,\n",
    "        \"reward_shaping\": False,\n",
    "        \"attention\": False,\n",
    "        \"hierarchical\": False,\n",
    "        \"change_priorities\": None  # No changes\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Flat_Dynamic\",\n",
    "        \"agent_class\": QLearningAgentFlat,\n",
    "        \"sparse_reward\": False,\n",
    "        \"reward_shaping\": False,\n",
    "        \"attention\": False,\n",
    "        \"hierarchical\": False,\n",
    "        \"change_priorities\": {\n",
    "            2500: {'X': 2, 'Y': 0, 'Z': 1},  # Change from X-Y-Z to Y-Z-X\n",
    "            #3500: {'X': 1, 'Y': 2, 'Z': 0},   # Change to Z-X-Y\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MaxInfoRL_Static\",\n",
    "        \"agent_class\": QLearningAgentMaxInfoRL_ADVANCED,\n",
    "        \"sparse_reward\": False,\n",
    "        \"reward_shaping\": False,\n",
    "        \"attention\": False,\n",
    "        \"hierarchical\": False,\n",
    "        \"change_priorities\": None  # No changes\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MaxInfoRL_Dynamic\",\n",
    "        \"agent_class\": QLearningAgentMaxInfoRL_ADVANCED,\n",
    "        \"sparse_reward\": False,\n",
    "        \"reward_shaping\": False,\n",
    "        \"attention\": False,\n",
    "        \"hierarchical\": False,\n",
    "        \"change_priorities\": {\n",
    "            2500: {'X': 2, 'Y': 0, 'Z': 1},  # Change from X-Y-Z to Y-Z-X\n",
    "            #3500: {'X': 1, 'Y': 2, 'Z': 0},   # Change to Z-X-Y\n",
    "\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Run experiments\n",
    "for exp in experiments:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Running experiment: {exp['name']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create environment\n",
    "    env = SARrobotEnv(\n",
    "        grid_rows=GRID_ROWS,\n",
    "        grid_cols=GRID_COLS,\n",
    "        info_number_needed=INFO_POINTS,\n",
    "        sparse_reward=exp[\"sparse_reward\"],\n",
    "        reward_shaping=exp[\"reward_shaping\"],\n",
    "        attention=exp[\"attention\"],\n",
    "        hierarchical=exp[\"hierarchical\"],\n",
    "        render_mode=None\n",
    "    )\n",
    "    \n",
    "    # Create agent\n",
    "    agent = exp[\"agent_class\"](\n",
    "        env=env,\n",
    "        ALPHA=ALPHA,\n",
    "        GAMMA=GAMMA,\n",
    "        EPSILON_MAX=EPSILON_MAX,\n",
    "        DECAY_RATE=DECAY_RATE,\n",
    "        EPSILON_MIN=EPSILON_MIN,\n",
    "        log_rewards_dir=os.path.join(LOG_DIR, exp[\"name\"]),\n",
    "        learned_policy_dir=os.path.join(POLICY_DIR, exp[\"name\"])\n",
    "    )\n",
    "    \n",
    "    # Train agent\n",
    "    rewards, steps, metrics = agent.train(NUM_EPISODES, change_priorities_at=exp[\"change_priorities\"])\n",
    "    \n",
    "    # Store results\n",
    "    results[exp[\"name\"]] = {\n",
    "        \"rewards\": rewards,\n",
    "        \"steps\": steps,\n",
    "        \"metrics\": metrics,\n",
    "        \"agent_class\": exp[\"agent_class\"].__name__\n",
    "    }\n",
    "\n",
    "# Save results\n",
    "with open(os.path.join(LOG_DIR, 'results.pkl'), 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualizations\n",
    "# 1. Reward trends - Separate plots for static and dynamic\n",
    "window_size = 20  # For smoothing\n",
    "\n",
    "# Static environment plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "for exp_name, data in results.items():\n",
    "    if \"Static\" in exp_name:  # Only include static experiments\n",
    "        rewards = data[\"rewards\"]\n",
    "        # Smooth rewards using moving average\n",
    "        smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "        plt.plot(smoothed_rewards, label=exp_name)\n",
    "\n",
    "plt.title('Reward Trends During Training (Static Environment)')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward (Smoothed)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(LOG_DIR, 'reward_trends_static.png'))\n",
    "\n",
    "# Dynamic environment plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "dynamic_smoothed_rewards = {}  # Store for min/max calculation\n",
    "\n",
    "for exp_name, data in results.items():\n",
    "    if \"Dynamic\" in exp_name:  # Only include dynamic experiments\n",
    "        rewards = data[\"rewards\"]\n",
    "        # Smooth rewards using moving average\n",
    "        smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "        plt.plot(smoothed_rewards, label=exp_name)\n",
    "        dynamic_smoothed_rewards[exp_name] = smoothed_rewards\n",
    "\n",
    "# Add priority change markers (if we have data for dynamic experiments)\n",
    "if dynamic_smoothed_rewards:\n",
    "    # Calculate global min/max for consistent text placement\n",
    "    all_rewards = np.concatenate(list(dynamic_smoothed_rewards.values()))\n",
    "    min_reward = np.min(all_rewards)\n",
    "    max_reward = np.max(all_rewards)\n",
    "    \n",
    "    # Get the change priority episodes from one of the dynamic experiments\n",
    "    for exp in experiments:\n",
    "        if exp[\"change_priorities\"] is not None:\n",
    "            for episode in exp[\"change_priorities\"].keys():\n",
    "                if episode >= window_size//2:\n",
    "                    adjusted_episode = episode - window_size//2\n",
    "                    plt.axvline(x=adjusted_episode, color='r', linestyle='--', alpha=0.5)\n",
    "                    plt.text(adjusted_episode, min_reward + (max_reward-min_reward)*0.1, \n",
    "                            f\"Priority\\nChange\", rotation=90, color='r', alpha=0.7)\n",
    "            break  # Only need one experiment's change points as they're the same\n",
    "\n",
    "plt.title('Reward Trends During Training (Dynamic Environment)')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward (Smoothed)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(LOG_DIR, 'reward_trends_dynamic.png'))\n",
    "\n",
    "# 2. Adaptation metrics comparison (for dynamic experiments)\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Extract adaptation metrics\n",
    "flat_changes = []\n",
    "maxinfo_changes = []\n",
    "\n",
    "for exp_name, data in results.items():\n",
    "    if \"Dynamic\" in exp_name and \"priority_changes\" in data[\"metrics\"]:\n",
    "        changes = data[\"metrics\"][\"priority_changes\"]\n",
    "        if \"Flat\" in exp_name:\n",
    "            flat_changes = changes\n",
    "        else:\n",
    "            maxinfo_changes = changes\n",
    "\n",
    "# Compare adaptation metrics side by side\n",
    "if flat_changes and maxinfo_changes:\n",
    "    # Setup plot for adaptation time\n",
    "    plt.subplot(2, 1, 1)\n",
    "    labels = [f\"Change {i+1}\" for i in range(min(len(flat_changes), len(maxinfo_changes)))]\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Extract steps to adapt\n",
    "    flat_steps = [change.get('steps_to_adapt', 0) for change in flat_changes[:len(labels)]]\n",
    "    maxinfo_steps = [change.get('steps_to_adapt', 0) for change in maxinfo_changes[:len(labels)]]\n",
    "    \n",
    "    plt.bar(x - width/2, flat_steps, width, label='Flat Agent')\n",
    "    plt.bar(x + width/2, maxinfo_steps, width, label='MaxInfoRL Agent')\n",
    "    plt.ylabel('Steps to Adapt')\n",
    "    plt.title('Adaptation Time After Priority Changes')\n",
    "    plt.xticks(x, labels)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Setup plot for success rate improvement\n",
    "    plt.subplot(2, 1, 2)\n",
    "    \n",
    "    # Calculate success rate improvement (after - before)\n",
    "    flat_improvements = [change.get('success_rate_after', 0) - change.get('success_rate_before', 0) \n",
    "                        for change in flat_changes[:len(labels)]]\n",
    "    maxinfo_improvements = [change.get('success_rate_after', 0) - change.get('success_rate_before', 0) \n",
    "                           for change in maxinfo_changes[:len(labels)]]\n",
    "    \n",
    "    plt.bar(x - width/2, flat_improvements, width, label='Flat Agent')\n",
    "    plt.bar(x + width/2, maxinfo_improvements, width, label='MaxInfoRL Agent')\n",
    "    plt.ylabel('Success Rate Improvement (%)')\n",
    "    plt.title('Performance Improvement After Adaptation')\n",
    "    plt.xticks(x, labels)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(LOG_DIR, 'adaptation_metrics.png'))\n",
    "\n",
    "# 3. Overall performance comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Key metrics to compare\n",
    "metrics_to_compare = [\n",
    "    'mission_success_rate', \n",
    "    'info_collection_success_rate',\n",
    "    'average_steps_per_episode',\n",
    "    'mission_success_no_collisions_rate'\n",
    "]\n",
    "\n",
    "metric_labels = {\n",
    "    'mission_success_rate': 'Mission Success (%)',\n",
    "    'info_collection_success_rate': 'Info Collection (%)',\n",
    "    'average_steps_per_episode': 'Avg Steps',\n",
    "    'mission_success_no_collisions_rate': 'Success Without Collisions (%)'\n",
    "}\n",
    "\n",
    "# Setup bar chart\n",
    "x = np.arange(len(metrics_to_compare))\n",
    "width = 0.2\n",
    "exp_names = list(results.keys())\n",
    "\n",
    "for i, exp_name in enumerate(exp_names):\n",
    "    values = [results[exp_name]['metrics'].get(metric, 0) for metric in metrics_to_compare]\n",
    "    offset = width * (i - len(exp_names)/2 + 0.5)\n",
    "    plt.bar(x + offset, values, width, label=exp_name)\n",
    "\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Performance Comparison Across Experiments')\n",
    "plt.xticks(x, [metric_labels[metric] for metric in metrics_to_compare])\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=len(exp_names))\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(LOG_DIR, 'performance_comparison.png'))\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"PERFORMANCE COMPARISON SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Metric':<40} | {'Flat_Static':<15} | {'Flat_Dynamic':<15} | {'MaxInfoRL_Static':<15} | {'MaxInfoRL_Dynamic':<15}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "metrics_to_print = [\n",
    "    'mission_success_rate',\n",
    "    'info_collection_success_rate', \n",
    "    'mission_success_no_collisions_rate',\n",
    "    'average_steps_per_episode',\n",
    "    'average_reward_per_episode',\n",
    "    'exploration_exploitation_ratio'\n",
    "]\n",
    "\n",
    "for metric in metrics_to_print:\n",
    "    values = []\n",
    "    for exp_name in [\"Flat_Static\", \"Flat_Dynamic\", \"MaxInfoRL_Static\", \"MaxInfoRL_Dynamic\"]:\n",
    "        if exp_name in results:\n",
    "            value = results[exp_name][\"metrics\"].get(metric, \"N/A\")\n",
    "            if isinstance(value, (int, float)):\n",
    "                values.append(f\"{value:.2f}\")\n",
    "            else:\n",
    "                values.append(str(value))\n",
    "        else:\n",
    "            values.append(\"N/A\")\n",
    "    \n",
    "    metric_name = metric_labels.get(metric, metric)\n",
    "    print(f\"{metric_name:<40} | {values[0]:<15} | {values[1]:<15} | {values[2]:<15} | {values[3]:<15}\")\n",
    "\n",
    "# Print adaptation summary for dynamic experiments\n",
    "if flat_changes and maxinfo_changes:\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"ADAPTATION METRICS SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"{'Metric':<30} | {'Flat_Dynamic':<15} | {'MaxInfoRL_Dynamic':<15} | {'Improvement':<15}\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Average steps to adapt\n",
    "    flat_avg_steps = np.mean([change.get('steps_to_adapt', 0) for change in flat_changes if change.get('adaptation_completed', False)])\n",
    "    maxinfo_avg_steps = np.mean([change.get('steps_to_adapt', 0) for change in maxinfo_changes if change.get('adaptation_completed', False)])\n",
    "    step_diff = maxinfo_avg_steps - flat_avg_steps\n",
    "    step_pct = (flat_avg_steps - maxinfo_avg_steps) / flat_avg_steps * 100 if flat_avg_steps > 0 else 0\n",
    "    step_sign = \"+\" if step_pct > 0 else \"\"\n",
    "    \n",
    "    print(f\"{'Avg Steps to Adapt':<30} | {flat_avg_steps:.1f}:<15 | {maxinfo_avg_steps:.1f}:<15 | {step_sign}{step_pct:.1f}%\")\n",
    "    \n",
    "    # Average success rate improvement\n",
    "    flat_avg_improve = np.mean([change.get('success_rate_after', 0) - change.get('success_rate_before', 0) \n",
    "                               for change in flat_changes if change.get('adaptation_completed', False)])\n",
    "    maxinfo_avg_improve = np.mean([change.get('success_rate_after', 0) - change.get('success_rate_before', 0) \n",
    "                                  for change in maxinfo_changes if change.get('adaptation_completed', False)])\n",
    "    improve_diff = maxinfo_avg_improve - flat_avg_improve\n",
    "    improve_sign = \"+\" if improve_diff > 0 else \"\"\n",
    "    \n",
    "    print(f\"{'Avg Success Rate Improvement':<30} | {flat_avg_improve:.1f}%:<15 | {maxinfo_avg_improve:.1f}%:<15 | {improve_sign}{improve_diff:.1f}%\")\n",
    "\n",
    "print(\"\\nExperiment complete! Results saved to:\", LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import evaluate_trained_policy\n",
    "evaluate_trained_policy(agent, \"logs/comparison_20250428_134413/policies/MaxInfoRL_Dynamic/q_extrinsic_table_episode_5000.npy\")\n",
    "# evaluate_trained_policy(agent_maxinfo, \"policies/ADV/q_extrinsic_table_episode_5000.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR TESTING ### newwww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1) Q-learning-flat -- TESTING\n",
    "env = SARrobotEnv(\n",
    "            grid_rows=4,\n",
    "            grid_cols=4,\n",
    "            info_number_needed=3,\n",
    "            sparse_reward=False,\n",
    "            reward_shaping=False,\n",
    "            attention=False,\n",
    "            hierarchical=False,\n",
    "            render_mode='None'\n",
    "        )\n",
    "all_total_rewards_AGENT_flat = []  # List to store total rewards from each run\n",
    "all_total_steps_AGENT_flat = []  # List to store total rewards from each run\n",
    "all_metrics_AGENT_flat = [] # New list to store metrics from each run\n",
    "\n",
    "for _ in range(param.testing_runs):\n",
    "    EPISODES = param.EPISODES\n",
    "    ALPHA = param.ALPHA\n",
    "    GAMMA = param.GAMMA\n",
    "    EPSILON_MAX = param.EPSILON_MAX\n",
    "    EPSILON_MIN = 0.1\n",
    "    DECAY_RATE = param.DECAY_RATE\n",
    "    agent_flat = QLearningAgentFlat(env, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN,\n",
    "                                    log_rewards_dir=\"curves/flat\", learned_policy_dir=\"policies/flat\")\n",
    "    rewards_flat, steps_flat, metrics_flat = agent_flat.train(5000)\n",
    "\n",
    "    all_total_rewards_AGENT_flat.append(rewards_flat)\n",
    "    all_total_steps_AGENT_flat.append(steps_flat)\n",
    "    all_metrics_AGENT_flat.append(metrics_flat)  # Store the metrics\n",
    "# save_training_results(agent_config['labels'][0], all_total_rewards_AGENT_flat, all_total_steps_AGENT_flat, all_metrics_AGENT_flat, save_dir='saved_results_no_sparse')\n",
    "# time.sleep(param.sleeping_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SARrobotEnv(\n",
    "            grid_rows=4,\n",
    "            grid_cols=4,\n",
    "            info_number_needed=3,\n",
    "            sparse_reward=False,\n",
    "            reward_shaping=False,\n",
    "            attention=False,\n",
    "            hierarchical=False,\n",
    "            render_mode='None'\n",
    "        )\n",
    "all_total_rewards_AGENTmaxinfo = []  # List to store total rewards from each run\n",
    "all_total_steps_AGENTmaxinfo = []  # List to store total rewards from each run\n",
    "all_metrics_AGENTmaxinfo = [] # New list to store metrics from each run\n",
    "\n",
    "for _ in range(param.testing_runs):\n",
    "    EPISODES = param.EPISODES\n",
    "    ALPHA = param.ALPHA\n",
    "    GAMMA = param.GAMMA\n",
    "    EPSILON_MAX = param.EPSILON_MAX\n",
    "    EPSILON_MIN = 0.1\n",
    "    DECAY_RATE = param.DECAY_RATE\n",
    "    agent_maxinfo = QLearningAgentMaxInfoRL_ADVANCED(env, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN,\n",
    "                                    log_rewards_dir=\"curves/ADV\", learned_policy_dir=\"policies/ADV\")\n",
    "    rewards_maxinfo, steps_maxinfo, metrics_maxinfo = agent_maxinfo.train(5000)\n",
    "\n",
    "    all_total_rewards_AGENTmaxinfo.append(rewards_maxinfo)\n",
    "    all_total_steps_AGENTmaxinfo.append(steps_maxinfo)\n",
    "    all_metrics_AGENTmaxinfo.append(metrics_maxinfo)  # Store the metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
