{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from datetime import datetime\n",
    "# import pickle\n",
    "# from environment_sar import SARrobotEnv\n",
    "# from agents import QLearningAgentFlat, QLearningAgentMaxInfoRL\n",
    "\n",
    "# # Configuration\n",
    "# GRID_ROWS = 4\n",
    "# GRID_COLS = 4\n",
    "# INFO_POINTS = 3  # Number of information points to collect\n",
    "# NUM_EPISODES = 5000\n",
    "# NUM_RUNS = 5  # Number of runs for each experiment\n",
    "# LOG_DIR = \"./logs/comparison_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# POLICY_DIR = os.path.join(LOG_DIR, \"policies\")\n",
    "\n",
    "# # Create output directories\n",
    "# os.makedirs(LOG_DIR, exist_ok=True)\n",
    "# os.makedirs(POLICY_DIR, exist_ok=True)\n",
    "\n",
    "# # RL parameters\n",
    "# ALPHA = 0.1\n",
    "# GAMMA = 0.99\n",
    "# EPSILON_MAX = 1.0\n",
    "# DECAY_RATE = 2\n",
    "# EPSILON_MIN = 0.1\n",
    "\n",
    "# # Define experiments\n",
    "# experiments = [\n",
    "#     {\n",
    "#         \"name\": \"Flat_Static\",\n",
    "#         \"agent_class\": QLearningAgentFlat,\n",
    "#         \"sparse_reward\": False,\n",
    "#         \"reward_shaping\": False,\n",
    "#         \"attention\": False,\n",
    "#         \"hierarchical\": False,\n",
    "#         \"change_priorities\": None  # No changes\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"Flat_Dynamic\",\n",
    "#         \"agent_class\": QLearningAgentFlat,\n",
    "#         \"sparse_reward\": False,\n",
    "#         \"reward_shaping\": False,\n",
    "#         \"attention\": False,\n",
    "#         \"hierarchical\": False,\n",
    "#         \"change_priorities\": {\n",
    "#             1700: {'X': 2, 'Y': 0, 'Z': 1},  # Change from X-Y-Z to Y-Z-X\n",
    "#             3500: {'X': 0, 'Y': 2, 'Z': 1},  # Change to Z-X-Y\n",
    "#         }\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"MaxInfoRL_Static\",\n",
    "#         \"agent_class\": QLearningAgentMaxInfoRL,\n",
    "#         \"sparse_reward\": False,\n",
    "#         \"reward_shaping\": False,\n",
    "#         \"attention\": False,\n",
    "#         \"hierarchical\": False,\n",
    "#         \"change_priorities\": None  # No changes\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"MaxInfoRL_Dynamic\",\n",
    "#         \"agent_class\": QLearningAgentMaxInfoRL,\n",
    "#         \"sparse_reward\": False,\n",
    "#         \"reward_shaping\": False,\n",
    "#         \"attention\": False,\n",
    "#         \"hierarchical\": False,\n",
    "#         \"change_priorities\": {\n",
    "#             1700: {'X': 2, 'Y': 0, 'Z': 1},  # Change from X-Y-Z to Y-Z-X\n",
    "#             3500: {'X': 0, 'Y': 2, 'Z': 1},  # Change to Z-X-Y\n",
    "#         }\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# results = {}\n",
    "\n",
    "# # Run experiments\n",
    "# for exp in experiments:\n",
    "#     print(f\"\\n{'='*80}\")\n",
    "#     print(f\"Running experiment: {exp['name']}\")\n",
    "#     print(f\"{'='*80}\")\n",
    "    \n",
    "#     # Initialize lists to store results from multiple runs\n",
    "#     all_rewards = []\n",
    "#     all_steps = []\n",
    "#     all_metrics = []\n",
    "    \n",
    "#     # Run multiple iterations of each experiment\n",
    "#     for run in range(NUM_RUNS):\n",
    "#         print(f\"Starting run {run+1}/{NUM_RUNS}\")\n",
    "        \n",
    "#         # Create environment\n",
    "#         env = SARrobotEnv(\n",
    "#             grid_rows=GRID_ROWS,\n",
    "#             grid_cols=GRID_COLS,\n",
    "#             info_number_needed=INFO_POINTS,\n",
    "#             sparse_reward=exp[\"sparse_reward\"],\n",
    "#             reward_shaping=exp[\"reward_shaping\"],\n",
    "#             attention=exp[\"attention\"],\n",
    "#             hierarchical=exp[\"hierarchical\"],\n",
    "#             render_mode=None\n",
    "#         )\n",
    "        \n",
    "#         # Create agent with run-specific log directory\n",
    "#         run_log_dir = os.path.join(LOG_DIR, exp[\"name\"], f\"run_{run+1}\")\n",
    "#         run_policy_dir = os.path.join(POLICY_DIR, exp[\"name\"], f\"run_{run+1}\")\n",
    "        \n",
    "#         # Create directories for this run\n",
    "#         os.makedirs(run_log_dir, exist_ok=True)\n",
    "#         os.makedirs(run_policy_dir, exist_ok=True)\n",
    "        \n",
    "#         agent = exp[\"agent_class\"](\n",
    "#             env=env,\n",
    "#             ALPHA=ALPHA,\n",
    "#             GAMMA=GAMMA,\n",
    "#             EPSILON_MAX=EPSILON_MAX,\n",
    "#             DECAY_RATE=DECAY_RATE,\n",
    "#             EPSILON_MIN=EPSILON_MIN,\n",
    "#             log_rewards_dir=run_log_dir,\n",
    "#             learned_policy_dir=run_policy_dir\n",
    "#         )\n",
    "        \n",
    "#         # Train agent\n",
    "#         rewards, steps, metrics = agent.train(NUM_EPISODES, change_priorities_at=exp[\"change_priorities\"])\n",
    "        \n",
    "#         # Store results for this run\n",
    "#         all_rewards.append(rewards)\n",
    "#         all_steps.append(steps)\n",
    "#         all_metrics.append(metrics)\n",
    "        \n",
    "#         # Save individual run results\n",
    "#         with open(os.path.join(run_log_dir, 'run_results.pkl'), 'wb') as f:\n",
    "#             pickle.dump({\n",
    "#                 \"rewards\": rewards,\n",
    "#                 \"steps\": steps,\n",
    "#                 \"metrics\": metrics,\n",
    "#                 \"agent_class\": exp[\"agent_class\"].__name__\n",
    "#             }, f)\n",
    "    \n",
    "#     # Process and average the results across runs\n",
    "#     avg_rewards = np.mean(all_rewards, axis=0)\n",
    "#     avg_steps = np.mean(all_steps, axis=0)\n",
    "    \n",
    "#     # Average the scalar metrics\n",
    "#     avg_metrics = {}\n",
    "#     for metric_key in all_metrics[0].keys():\n",
    "#         # Skip non-numeric or nested metrics for simple averaging\n",
    "#         if metric_key in ['collection_stats', 'predictor_stats', 'priority_changes', 'collision_counts_per_successful_episode']:\n",
    "#             avg_metrics[metric_key] = all_metrics[0][metric_key]  # Just use the first run's data for these\n",
    "#             continue\n",
    "            \n",
    "#         # For numeric metrics, calculate the average\n",
    "#         if isinstance(all_metrics[0][metric_key], (int, float)):\n",
    "#             avg_metrics[metric_key] = np.mean([m[metric_key] for m in all_metrics])\n",
    "#         else:\n",
    "#             avg_metrics[metric_key] = all_metrics[0][metric_key]\n",
    "    \n",
    "#     # For priority_changes, we need to average adaptation metrics\n",
    "#     if 'priority_changes' in all_metrics[0] and all_metrics[0]['priority_changes']:\n",
    "#         # Initialize averaged priority changes data structure\n",
    "#         avg_priority_changes = []\n",
    "        \n",
    "#         # For each change point (assuming same number across runs)\n",
    "#         for change_idx in range(len(all_metrics[0]['priority_changes'])):\n",
    "#             # Extract data for this change point from each run\n",
    "#             changes_data = []\n",
    "#             for run_metrics in all_metrics:\n",
    "#                 if 'priority_changes' in run_metrics and len(run_metrics['priority_changes']) > change_idx:\n",
    "#                     changes_data.append(run_metrics['priority_changes'][change_idx])\n",
    "            \n",
    "#             # Calculate averages for this change point\n",
    "#             if changes_data:\n",
    "#                 avg_change = {\n",
    "#                     'episode': changes_data[0]['episode'],  # Use episode number from first run\n",
    "#                     'steps_to_adapt': np.mean([c.get('steps_to_adapt', 0) for c in changes_data if 'steps_to_adapt' in c]),\n",
    "#                     'success_rate_before': np.mean([c.get('success_rate_before', 0) for c in changes_data if 'success_rate_before' in c]),\n",
    "#                     'success_rate_after': np.mean([c.get('success_rate_after', 0) for c in changes_data if 'success_rate_after' in c]),\n",
    "#                     'order_alignment_rate': np.mean([c.get('order_alignment_rate', 0) for c in changes_data if 'order_alignment_rate' in c])\n",
    "#                 }\n",
    "#                 avg_priority_changes.append(avg_change)\n",
    "        \n",
    "#         avg_metrics['priority_changes'] = avg_priority_changes\n",
    "    \n",
    "#     # Store averaged results\n",
    "#     results[exp[\"name\"]] = {\n",
    "#         \"rewards\": avg_rewards,\n",
    "#         \"steps\": avg_steps,\n",
    "#         \"metrics\": avg_metrics,\n",
    "#         \"agent_class\": exp[\"agent_class\"].__name__,\n",
    "#         \"all_run_rewards\": all_rewards,  # Store all runs for additional analysis if needed\n",
    "#         \"all_run_steps\": all_steps,\n",
    "#         \"all_run_metrics\": all_metrics\n",
    "#     }\n",
    "\n",
    "# # Save final averaged results\n",
    "# with open(os.path.join(LOG_DIR, 'averaged_results.pkl'), 'wb') as f:\n",
    "#     pickle.dump(results, f)\n",
    "\n",
    "# print(\"\\nAll experiments completed. Generating visualizations...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create comparison visualizations with confidence intervals\n",
    "# window_size = 20  # For smoothing\n",
    "\n",
    "# # Static environment plot\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# for exp_name, data in results.items():\n",
    "#     if \"Static\" in exp_name:  # Only include static experiments\n",
    "#         rewards = data[\"rewards\"]\n",
    "#         # Smooth rewards using moving average\n",
    "#         smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "#         plt.plot(smoothed_rewards, label=exp_name)\n",
    "        \n",
    "#         # Add confidence intervals\n",
    "#         if \"all_run_rewards\" in data:\n",
    "#             all_smoothed = []\n",
    "#             for run_rewards in data[\"all_run_rewards\"]:\n",
    "#                 run_smoothed = np.convolve(run_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "#                 all_smoothed.append(run_smoothed)\n",
    "            \n",
    "#             all_smoothed = np.array(all_smoothed)\n",
    "#             std_dev = np.std(all_smoothed, axis=0)\n",
    "            \n",
    "#             # Plot confidence interval (±1 std dev)\n",
    "#             x = np.arange(len(smoothed_rewards))\n",
    "#             plt.fill_between(x, smoothed_rewards - std_dev, smoothed_rewards + std_dev, alpha=0.2)\n",
    "\n",
    "# plt.title('Reward Trends During Training (Static Environment)')\n",
    "# plt.xlabel('Episodes')\n",
    "# plt.ylabel('Average Reward (Smoothed)')\n",
    "# plt.legend()\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.savefig(os.path.join(LOG_DIR, 'reward_trends_static.png'))\n",
    "\n",
    "# # Dynamic environment plot\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# dynamic_smoothed_rewards = {}  # Store for min/max calculation\n",
    "\n",
    "# for exp_name, data in results.items():\n",
    "#     if \"Dynamic\" in exp_name:  # Only include dynamic experiments\n",
    "#         rewards = data[\"rewards\"]\n",
    "#         # Smooth rewards using moving average\n",
    "#         smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "#         plt.plot(smoothed_rewards, label=exp_name)\n",
    "#         dynamic_smoothed_rewards[exp_name] = smoothed_rewards\n",
    "        \n",
    "#         # Add confidence intervals\n",
    "#         if \"all_run_rewards\" in data:\n",
    "#             all_smoothed = []\n",
    "#             for run_rewards in data[\"all_run_rewards\"]:\n",
    "#                 run_smoothed = np.convolve(run_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "#                 all_smoothed.append(run_smoothed)\n",
    "            \n",
    "#             all_smoothed = np.array(all_smoothed)\n",
    "#             std_dev = np.std(all_smoothed, axis=0)\n",
    "            \n",
    "#             # Plot confidence interval (±1 std dev)\n",
    "#             x = np.arange(len(smoothed_rewards))\n",
    "#             plt.fill_between(x, smoothed_rewards - std_dev, smoothed_rewards + std_dev, alpha=0.2)\n",
    "\n",
    "# # Add priority change markers (if we have data for dynamic experiments)\n",
    "# if dynamic_smoothed_rewards:\n",
    "#     # Calculate global min/max for consistent text placement\n",
    "#     all_rewards = np.concatenate(list(dynamic_smoothed_rewards.values()))\n",
    "#     min_reward = np.min(all_rewards)\n",
    "#     max_reward = np.max(all_rewards)\n",
    "    \n",
    "#     # Get the change priority episodes from one of the dynamic experiments\n",
    "#     for exp in experiments:\n",
    "#         if exp[\"change_priorities\"] is not None:\n",
    "#             for episode in exp[\"change_priorities\"].keys():\n",
    "#                 if episode >= window_size//2:\n",
    "#                     adjusted_episode = episode - window_size//2\n",
    "                    \n",
    "#                     bullet_y = max_reward + (max_reward - min_reward) * 0.05  # Slightly above highest reward\n",
    "#                     plt.plot(adjusted_episode, \n",
    "#                             bullet_y, \n",
    "#                             marker='o', \n",
    "#                             markersize=10, \n",
    "#                             color='red', \n",
    "#                             label='Priority Change' if episode == list(exp[\"change_priorities\"].keys())[0] else \"\")\n",
    "#             break  # Only need one experiment's change points as they're the same\n",
    "\n",
    "# plt.title('Reward Trends During Training (Dynamic Environment)')\n",
    "# plt.xlabel('Episodes')\n",
    "# plt.ylabel('Average Reward (Smoothed)')\n",
    "# plt.legend()\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.savefig(os.path.join(LOG_DIR, 'reward_trends_dynamic.png'))\n",
    "\n",
    "# # 2. Adaptation metrics comparison (for dynamic experiments)\n",
    "# plt.figure(figsize=(10, 8))\n",
    "\n",
    "# # Extract adaptation metrics\n",
    "# flat_changes = []\n",
    "# maxinfo_changes = []\n",
    "\n",
    "# for exp_name, data in results.items():\n",
    "#     if \"Dynamic\" in exp_name and \"priority_changes\" in data[\"metrics\"]:\n",
    "#         changes = data[\"metrics\"][\"priority_changes\"]\n",
    "#         if \"Flat\" in exp_name:\n",
    "#             flat_changes = changes\n",
    "#         else:\n",
    "#             maxinfo_changes = changes\n",
    "\n",
    "# # Compare adaptation metrics side by side\n",
    "# if flat_changes and maxinfo_changes:\n",
    "#     # Setup plot for adaptation time\n",
    "#     plt.subplot(2, 1, 1)\n",
    "#     labels = [f\"Change {i+1}\" for i in range(min(len(flat_changes), len(maxinfo_changes)))]\n",
    "#     x = np.arange(len(labels))\n",
    "#     width = 0.35\n",
    "    \n",
    "#     # Extract steps to adapt\n",
    "#     flat_steps = [change.get('steps_to_adapt', 0) for change in flat_changes[:len(labels)]]\n",
    "#     maxinfo_steps = [change.get('steps_to_adapt', 0) for change in maxinfo_changes[:len(labels)]]\n",
    "    \n",
    "#     # Get standard deviations for error bars if available\n",
    "#     flat_steps_std = []\n",
    "#     maxinfo_steps_std = []\n",
    "    \n",
    "#     if \"all_run_metrics\" in results[\"Flat_Dynamic\"] and \"all_run_metrics\" in results[\"MaxInfoRL_Dynamic\"]:\n",
    "#         for i in range(len(labels)):\n",
    "#             # Flat agent std dev\n",
    "#             flat_run_steps = []\n",
    "#             for run_metrics in results[\"Flat_Dynamic\"][\"all_run_metrics\"]:\n",
    "#                 if \"priority_changes\" in run_metrics and i < len(run_metrics[\"priority_changes\"]):\n",
    "#                     steps = run_metrics[\"priority_changes\"][i].get('steps_to_adapt', 0)\n",
    "#                     flat_run_steps.append(steps)\n",
    "#             flat_steps_std.append(np.std(flat_run_steps) if flat_run_steps else 0)\n",
    "            \n",
    "#             # MaxInfoRL agent std dev\n",
    "#             maxinfo_run_steps = []\n",
    "#             for run_metrics in results[\"MaxInfoRL_Dynamic\"][\"all_run_metrics\"]:\n",
    "#                 if \"priority_changes\" in run_metrics and i < len(run_metrics[\"priority_changes\"]):\n",
    "#                     steps = run_metrics[\"priority_changes\"][i].get('steps_to_adapt', 0)\n",
    "#                     maxinfo_run_steps.append(steps)\n",
    "#             maxinfo_steps_std.append(np.std(maxinfo_run_steps) if maxinfo_run_steps else 0)\n",
    "    \n",
    "#     plt.bar(x - width/2, flat_steps, width, label='Flat Agent', yerr=flat_steps_std if flat_steps_std else None)\n",
    "#     plt.bar(x + width/2, maxinfo_steps, width, label='MaxInfoRL Agent', yerr=maxinfo_steps_std if maxinfo_steps_std else None)\n",
    "#     plt.ylabel('Steps to Adapt')\n",
    "#     plt.title('Adaptation Time After Priority Changes')\n",
    "#     plt.xticks(x, labels)\n",
    "#     plt.legend()\n",
    "    \n",
    "#     # Setup plot for success rate improvement\n",
    "#     plt.subplot(2, 1, 2)\n",
    "    \n",
    "#     # Calculate success rate improvement (after - before)\n",
    "#     flat_improvements = [change.get('success_rate_after', 0) - change.get('success_rate_before', 0) \n",
    "#                         for change in flat_changes[:len(labels)]]\n",
    "#     maxinfo_improvements = [change.get('success_rate_after', 0) - change.get('success_rate_before', 0) \n",
    "#                            for change in maxinfo_changes[:len(labels)]]\n",
    "    \n",
    "#     # Get standard deviations for error bars if available\n",
    "#     flat_improve_std = []\n",
    "#     maxinfo_improve_std = []\n",
    "    \n",
    "#     if \"all_run_metrics\" in results[\"Flat_Dynamic\"] and \"all_run_metrics\" in results[\"MaxInfoRL_Dynamic\"]:\n",
    "#         for i in range(len(labels)):\n",
    "#             # Flat agent std dev\n",
    "#             flat_run_improve = []\n",
    "#             for run_metrics in results[\"Flat_Dynamic\"][\"all_run_metrics\"]:\n",
    "#                 if \"priority_changes\" in run_metrics and i < len(run_metrics[\"priority_changes\"]):\n",
    "#                     before = run_metrics[\"priority_changes\"][i].get('success_rate_before', 0)\n",
    "#                     after = run_metrics[\"priority_changes\"][i].get('success_rate_after', 0)\n",
    "#                     flat_run_improve.append(after - before)\n",
    "#             flat_improve_std.append(np.std(flat_run_improve) if flat_run_improve else 0)\n",
    "            \n",
    "#             # MaxInfoRL agent std dev\n",
    "#             maxinfo_run_improve = []\n",
    "#             for run_metrics in results[\"MaxInfoRL_Dynamic\"][\"all_run_metrics\"]:\n",
    "#                 if \"priority_changes\" in run_metrics and i < len(run_metrics[\"priority_changes\"]):\n",
    "#                     before = run_metrics[\"priority_changes\"][i].get('success_rate_before', 0)\n",
    "#                     after = run_metrics[\"priority_changes\"][i].get('success_rate_after', 0)\n",
    "#                     maxinfo_run_improve.append(after - before)\n",
    "#             maxinfo_improve_std.append(np.std(maxinfo_run_improve) if maxinfo_run_improve else 0)\n",
    "    \n",
    "#     plt.bar(x - width/2, flat_improvements, width, label='Flat Agent', yerr=flat_improve_std if flat_improve_std else None)\n",
    "#     plt.bar(x + width/2, maxinfo_improvements, width, label='MaxInfoRL Agent', yerr=maxinfo_improve_std if maxinfo_improve_std else None)\n",
    "#     plt.ylabel('Success Rate Improvement (%)')\n",
    "#     plt.title('Performance Improvement After Adaptation')\n",
    "#     plt.xticks(x, labels)\n",
    "#     plt.legend()\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(os.path.join(LOG_DIR, 'adaptation_metrics.png'))\n",
    "\n",
    "# # 3. Overall performance comparison with error bars\n",
    "# plt.figure(figsize=(12, 6))\n",
    "\n",
    "# # Key metrics to compare\n",
    "# metrics_to_compare = [\n",
    "#     'mission_success_rate', \n",
    "#     'info_collection_success_rate',\n",
    "#     'average_steps_per_episode',\n",
    "#     'mission_success_no_collisions_rate'\n",
    "# ]\n",
    "\n",
    "# metric_labels = {\n",
    "#     'mission_success_rate': 'Mission Success (%)',\n",
    "#     'info_collection_success_rate': 'Info Collection (%)',\n",
    "#     'average_steps_per_episode': 'Avg Steps',\n",
    "#     'mission_success_no_collisions_rate': 'Success Without Collisions (%)'\n",
    "# }\n",
    "\n",
    "# # Setup bar chart\n",
    "# x = np.arange(len(metrics_to_compare))\n",
    "# width = 0.2\n",
    "# exp_names = list(results.keys())\n",
    "\n",
    "# for i, exp_name in enumerate(exp_names):\n",
    "#     values = [results[exp_name]['metrics'].get(metric, 0) for metric in metrics_to_compare]\n",
    "#     offset = width * (i - len(exp_names)/2 + 0.5)\n",
    "    \n",
    "#     # Calculate standard deviations for error bars if available\n",
    "#     error_bars = None\n",
    "#     if \"all_run_metrics\" in results[exp_name]:\n",
    "#         stds = []\n",
    "#         for metric in metrics_to_compare:\n",
    "#             metric_values = [run_metrics.get(metric, 0) for run_metrics in results[exp_name][\"all_run_metrics\"]]\n",
    "#             stds.append(np.std(metric_values))\n",
    "#         error_bars = stds\n",
    "    \n",
    "#     plt.bar(x + offset, values, width, label=exp_name, yerr=error_bars)\n",
    "\n",
    "# plt.xlabel('Metrics')\n",
    "# plt.ylabel('Value')\n",
    "# plt.title('Performance Comparison Across Experiments')\n",
    "# plt.xticks(x, [metric_labels[metric] for metric in metrics_to_compare])\n",
    "# plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=len(exp_names))\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(os.path.join(LOG_DIR, 'performance_comparison.png'))\n",
    "\n",
    "# # Print summary table\n",
    "# print(\"\\n\" + \"=\"*100)\n",
    "# print(\"PERFORMANCE COMPARISON SUMMARY (AVERAGED OVER 5 RUNS)\")\n",
    "# print(\"=\"*100)\n",
    "# print(f\"{'Metric':<40} | {'Flat_Static':<15} | {'Flat_Dynamic':<15} | {'MaxInfoRL_Static':<15} | {'MaxInfoRL_Dynamic':<15}\")\n",
    "# print(\"=\"*100)\n",
    "\n",
    "# metrics_to_print = [\n",
    "#     'mission_success_rate',\n",
    "#     'info_collection_success_rate', \n",
    "#     'mission_success_no_collisions_rate',\n",
    "#     'average_steps_per_episode',\n",
    "#     'average_reward_per_episode',\n",
    "#     'exploration_exploitation_ratio'\n",
    "# ]\n",
    "\n",
    "# for metric in metrics_to_print:\n",
    "#     values = []\n",
    "#     for exp_name in [\"Flat_Static\", \"Flat_Dynamic\", \"MaxInfoRL_Static\", \"MaxInfoRL_Dynamic\"]:\n",
    "#         if exp_name in results:\n",
    "#             value = results[exp_name][\"metrics\"].get(metric, \"N/A\")\n",
    "#             if isinstance(value, (int, float)):\n",
    "#                 values.append(f\"{value:.2f}\")\n",
    "#             else:\n",
    "#                 values.append(str(value))\n",
    "#         else:\n",
    "#             values.append(\"N/A\")\n",
    "    \n",
    "#     metric_name = metric_labels.get(metric, metric)\n",
    "#     print(f\"{metric_name:<40} | {values[0]:<15} | {values[1]:<15} | {values[2]:<15} | {values[3]:<15}\")\n",
    "\n",
    "# print(\"\\nExperiment complete! Results saved to:\", LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import random\n",
    "from environment_sar import SARrobotEnv, searchANDrescueRobot\n",
    "from InformationCollection import InfoLocation, InfoCollectionSystem\n",
    "from agents import QLearningAgentFlat, QLearningAgentMaxInfoRL, QLearningAgentFlat_Boosted\n",
    "from robot_utils import get_file_type, RobotOption\n",
    "from LLM_ContextExtractor import DisasterResponseAssistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import random\n",
    "from environment_sar import SARrobotEnv, searchANDrescueRobot\n",
    "from InformationCollection import InfoLocation, InfoCollectionSystem\n",
    "from agents import QLearningAgentFlat, QLearningAgentMaxInfoRL, QLearningAgentFlat_Boosted\n",
    "from robot_utils import get_file_type, RobotOption\n",
    "from LLM_ContextExtractor import DisasterResponseAssistant\n",
    "\n",
    "# Create configurable robot class that extends the original\n",
    "class ConfigurableRobot(searchANDrescueRobot):\n",
    "    def __init__(self, grid_rows, grid_cols, info_number_needed, attention, hierarchical, env_config=None):\n",
    "        # Initialize base attributes\n",
    "        self.grid_rows = grid_rows\n",
    "        self.grid_cols = grid_cols\n",
    "        self.hierarchical = hierarchical\n",
    "        self.attention = attention\n",
    "        self.info_number_needed = info_number_needed\n",
    "        \n",
    "        # Store the original config for reset\n",
    "        self.original_config = None\n",
    "        \n",
    "        # Use provided config if available\n",
    "        if env_config:\n",
    "            # Store a deep copy of the original config\n",
    "            self.original_config = pickle.loads(pickle.dumps(env_config))\n",
    "            \n",
    "            self.init_positions = env_config['init_positions']\n",
    "            self.target_pos = env_config['target_pos']\n",
    "            self.ditches = env_config['ditches']\n",
    "            self.GENERAL_FIRES_UNKNOWN_TO_THE_AGENT = env_config['hazards']\n",
    "            \n",
    "            # Convert dictionaries to InfoLocation objects\n",
    "            info_locations = []\n",
    "            for loc_data in env_config['info_locations']:\n",
    "                info_locations.append(InfoLocation(\n",
    "                    position=loc_data['position'],\n",
    "                    info_type=loc_data['info_type'],\n",
    "                    collection_order=loc_data['collection_order'],\n",
    "                    required=True\n",
    "                ))\n",
    "            \n",
    "            # Print for debugging\n",
    "            print(f\"Robot initialized with configuration:\")\n",
    "            print(f\"Initial positions: {self.init_positions}\")\n",
    "            print(f\"Target position: {self.target_pos}\")\n",
    "            print(f\"Info locations: {[(loc.position, loc.info_type) for loc in info_locations]}\")\n",
    "            \n",
    "            # Initialize info system with custom locations\n",
    "            self.info_system = InfoCollectionSystem(info_locations)\n",
    "        else:\n",
    "            # Call the original __init__ to set up with default configuration\n",
    "            super().__init__(grid_rows, grid_cols, info_number_needed, attention, hierarchical)\n",
    "            return\n",
    "        \n",
    "        # Initialize remaining attributes similar to the original class\n",
    "        self.ask_action_counter = 0\n",
    "        self.visited_information_state = False\n",
    "        self.input_received = False\n",
    "        self.POIs, self.fires, self.hazards, self.poi = [], [], [], []\n",
    "        self.sensor_readings = {}\n",
    "        self.visited_pois = set()\n",
    "        \n",
    "        # Action tracking at info locations\n",
    "        self.info_location_actions = {tuple(loc.position): 0 for loc in info_locations}\n",
    "        self.max_info_location_actions = 2\n",
    "        self.total_info_location_actions = 0\n",
    "\n",
    "        # Load the disaster response assistant\n",
    "        document_path = \"sar_data.json\"\n",
    "        document_type = get_file_type(document_path)\n",
    "        self.assistant = DisasterResponseAssistant(document_path, document_type)\n",
    "\n",
    "        # Add collision counter\n",
    "        self.hazard_collisions = 0\n",
    "        self.episode_collisions = 0\n",
    "        self._reset()\n",
    "        \n",
    "    def _reset(self, seed=None):\n",
    "        \"\"\"Override the reset method to ensure consistent initial setup\"\"\"\n",
    "        self.ask_action_counter = 0\n",
    "        \n",
    "        # Always use the first position to ensure consistency\n",
    "        if self.init_positions and len(self.init_positions) > 0:\n",
    "            # Use the first position instead of random choice\n",
    "            self.robot_pos = self.init_positions[0].copy()\n",
    "        \n",
    "        self.has_saved = 0\n",
    "        if self.hierarchical:\n",
    "            self.current_option = RobotOption.NAVIGATION.value\n",
    "            \n",
    "        # If we have an original config, restore from it\n",
    "        if self.original_config:\n",
    "            # Restore initial positions from original config if needed\n",
    "            if self.init_positions != self.original_config['init_positions']:\n",
    "                self.init_positions = self.original_config['init_positions']\n",
    "            \n",
    "            # Ensure target position is consistent\n",
    "            self.target_pos = self.original_config['target_pos']\n",
    "            \n",
    "            # Restore ditches and hazards\n",
    "            self.ditches = self.original_config['ditches']\n",
    "            self.GENERAL_FIRES_UNKNOWN_TO_THE_AGENT = self.original_config['hazards']\n",
    "       \n",
    "        # Reset other states\n",
    "        self.POIs, self.fires = [], []\n",
    "        self.visited_information_state = False\n",
    "        self.visited_pois = set()\n",
    "        \n",
    "        # Reset action tracking\n",
    "        self.info_location_actions = {tuple(loc.position): 0 for loc in self.info_system.info_locations}\n",
    "        self.max_info_location_actions = 2\n",
    "        self.total_info_location_actions = 0\n",
    "        self.episode_collisions = 0 # Reset collision counter for new episode\n",
    "        \n",
    "        # Also reset the info system\n",
    "        self.info_system.reset_episode()\n",
    "\n",
    "# Create configurable environment class\n",
    "class ConfigurableEnv(SARrobotEnv):\n",
    "    def __init__(self, grid_rows, grid_cols, info_number_needed, sparse_reward, reward_shaping, \n",
    "                attention, hierarchical, render_mode=None, env_config=None):\n",
    "        # Store parameters\n",
    "        self.grid_rows = grid_rows\n",
    "        self.grid_cols = grid_cols\n",
    "        self.render_mode = render_mode\n",
    "        self.info_number_needed = info_number_needed\n",
    "        self.sparse_reward = sparse_reward\n",
    "        self.reward_shaping = reward_shaping\n",
    "        self.hierarchical = hierarchical\n",
    "        self.attention = attention\n",
    "        self.env_config = env_config\n",
    "        \n",
    "        # Print environment configuration\n",
    "        print(\"\\nEnvironment Configuration:\")\n",
    "        \n",
    "        if env_config:\n",
    "            # Print the actual configuration values to verify\n",
    "            print(f\"Initial Position: {env_config['init_positions']}\")\n",
    "            print(f\"Target Position: {env_config['target_pos']}\")\n",
    "            print(f\"Info Locations: {[loc['position'] for loc in env_config['info_locations']]}\")\n",
    "        else:\n",
    "            print(\"Using default configuration\")\n",
    "            \n",
    "        print(f\"Sparse Reward Mode: {self.sparse_reward}\")\n",
    "        print(f\"Hierarchical Mode: {self.hierarchical}\")\n",
    "        print(f\"Reward Shaping Mode: {self.reward_shaping}\")\n",
    "        print(f\"Attention Mechanism: {self.attention}\")\n",
    "        print(f\"Grid Size: {self.grid_rows}x{self.grid_cols}\")\n",
    "        print(f\"Required Information Points: {self.info_number_needed}\\n\")\n",
    "        \n",
    "        # Create configurable robot\n",
    "        self.sar_robot = ConfigurableRobot(\n",
    "            self.grid_rows, \n",
    "            self.grid_cols, \n",
    "            self.info_number_needed, \n",
    "            self.attention, \n",
    "            self.hierarchical, \n",
    "            env_config=self.env_config\n",
    "        )\n",
    "        \n",
    "        # Set up the appropriate action space or option space\n",
    "        if self.hierarchical:\n",
    "            from gymnasium import spaces\n",
    "            from robot_utils import RobotOption\n",
    "            self.option_space = spaces.Discrete(len(RobotOption))\n",
    "        else:\n",
    "            from gymnasium import spaces\n",
    "            from robot_utils import RobotAction\n",
    "            self.action_space = spaces.Discrete(len(RobotAction))\n",
    "        \n",
    "        # Set up observation space\n",
    "        from gymnasium import spaces\n",
    "        required_info_count = self.sar_robot.info_system.get_required_info_count()\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=np.array([self.grid_rows-1, self.grid_cols-1, required_info_count, 1]),\n",
    "            shape=(4,),\n",
    "            dtype=np.int32\n",
    "        )\n",
    "        \n",
    "        # Environment parameters\n",
    "        self.max_steps = 50\n",
    "        self.current_step = 0\n",
    "        self.turnPenalty = -1\n",
    "        self.stepsPenalty = -5\n",
    "        self.ditchPenalty = -30\n",
    "        self.illegalActionPenalty = -5\n",
    "        self.infoLocationActionsPenalty = -5\n",
    "        self.winReward = 100\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        # Handle reward shaping mechanism if needed\n",
    "        self.reward_shaping_mechanism = None  # Initialize to None to avoid errors\n",
    "\n",
    "# Main experiment function\n",
    "def run_multi_env_experiments(env_configs, agent_types, num_trials, num_episodes):\n",
    "    \"\"\"\n",
    "    Run experiments with multiple environments and agent types.\n",
    "    \n",
    "    Args:\n",
    "        env_configs: List of environment configurations\n",
    "        agent_types: List of agent configurations\n",
    "        num_trials: Number of trials to run\n",
    "        num_episodes: Number of episodes to train each agent\n",
    "        \n",
    "    Returns:\n",
    "        all_results: Dictionary containing experiment results\n",
    "    \"\"\"\n",
    "    # Setup directories\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_dir = f\"./logs/multi_env_comparison_{timestamp}\"\n",
    "    policy_dir = os.path.join(log_dir, \"policies\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(policy_dir, exist_ok=True)\n",
    "    \n",
    "    # RL parameters\n",
    "    alpha = 0.1\n",
    "    gamma = 0.99\n",
    "    epsilon_max = 1.0\n",
    "    decay_rate = 2\n",
    "    epsilon_min = 0.1\n",
    "    \n",
    "    # Initialize results\n",
    "    all_results = {}\n",
    "    config_counts = {i: 0 for i in range(len(env_configs))}\n",
    "    \n",
    "    # Set random seed for reproducibility across trials\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Run trials\n",
    "    for trial in range(num_trials):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Starting Trial {trial+1}/{num_trials}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Randomly select environment configuration\n",
    "        config_idx = random.randint(0, len(env_configs) - 1)\n",
    "        selected_config = env_configs[config_idx]\n",
    "        config_counts[config_idx] += 1\n",
    "        \n",
    "        print(f\"Selected Environment Configuration: {config_idx + 1}\")\n",
    "        print(f\"Initial Position: {selected_config['init_positions']}\")\n",
    "        print(f\"Target Position: {selected_config['target_pos']}\")\n",
    "        print(f\"Info Locations: {[loc['position'] for loc in selected_config['info_locations']]}\")\n",
    "        \n",
    "        # Create trial directory\n",
    "        trial_dir = os.path.join(log_dir, f\"trial_{trial+1}\")\n",
    "        os.makedirs(trial_dir, exist_ok=True)\n",
    "        \n",
    "        # Save configuration\n",
    "        with open(os.path.join(trial_dir, 'config.pkl'), 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'config_idx': config_idx,\n",
    "                'config': selected_config\n",
    "            }, f)\n",
    "        \n",
    "        # Results for this trial\n",
    "        trial_results = {}\n",
    "        \n",
    "        # Run each agent type\n",
    "        for agent_type in agent_types:\n",
    "            print(f\"\\nRunning agent: {agent_type['name']}\")\n",
    "            \n",
    "            # Make a deep copy of the selected config for each agent to prevent modification\n",
    "            agent_config = pickle.loads(pickle.dumps(selected_config))\n",
    "            \n",
    "            # Create environment with a fresh copy of the selected configuration\n",
    "            env = ConfigurableEnv(\n",
    "                grid_rows=4,\n",
    "                grid_cols=4,\n",
    "                info_number_needed=3,\n",
    "                sparse_reward=agent_type[\"sparse_reward\"],\n",
    "                reward_shaping=agent_type[\"reward_shaping\"],\n",
    "                attention=agent_type[\"attention\"],\n",
    "                hierarchical=agent_type[\"hierarchical\"],\n",
    "                render_mode=None,\n",
    "                env_config=agent_config  # Use a fresh copy for each agent\n",
    "            )\n",
    "            \n",
    "            # Setup agent directories\n",
    "            agent_log_dir = os.path.join(trial_dir, agent_type[\"name\"])\n",
    "            agent_policy_dir = os.path.join(trial_dir, \"policies\", agent_type[\"name\"])\n",
    "            os.makedirs(agent_log_dir, exist_ok=True)\n",
    "            os.makedirs(agent_policy_dir, exist_ok=True)\n",
    "            \n",
    "            # Create agent\n",
    "            agent = agent_type[\"agent_class\"](\n",
    "                env=env,\n",
    "                ALPHA=alpha,\n",
    "                GAMMA=gamma,\n",
    "                EPSILON_MAX=epsilon_max,\n",
    "                DECAY_RATE=decay_rate,\n",
    "                EPSILON_MIN=epsilon_min,\n",
    "                log_rewards_dir=agent_log_dir,\n",
    "                learned_policy_dir=agent_policy_dir\n",
    "            )\n",
    "            \n",
    "            # Train agent\n",
    "            rewards, steps, metrics = agent.train(num_episodes, change_priorities_at=agent_type[\"change_priorities\"])\n",
    "            \n",
    "            # Store results\n",
    "            trial_results[agent_type[\"name\"]] = {\n",
    "                \"rewards\": rewards,\n",
    "                \"steps\": steps,\n",
    "                \"metrics\": metrics,\n",
    "                \"agent_class\": agent_type[\"agent_class\"].__name__\n",
    "            }\n",
    "            \n",
    "            # Save agent results\n",
    "            with open(os.path.join(agent_log_dir, 'results.pkl'), 'wb') as f:\n",
    "                pickle.dump(trial_results[agent_type[\"name\"]], f)\n",
    "        \n",
    "        # Save trial results\n",
    "        with open(os.path.join(trial_dir, 'trial_results.pkl'), 'wb') as f:\n",
    "            pickle.dump(trial_results, f)\n",
    "        \n",
    "        # Update overall results\n",
    "        all_results[f\"trial_{trial+1}\"] = {\n",
    "            \"config_idx\": config_idx,\n",
    "            \"results\": trial_results\n",
    "        }\n",
    "    \n",
    "    # Save final results\n",
    "    with open(os.path.join(log_dir, 'all_results.pkl'), 'wb') as f:\n",
    "        pickle.dump(all_results, f)\n",
    "    \n",
    "    print(\"\\nConfiguration selection counts:\")\n",
    "    for idx, count in config_counts.items():\n",
    "        print(f\"Config {idx+1}: selected {count} times\")\n",
    "    \n",
    "    # Generate visualizations\n",
    "    # generate_summary_visualizations(all_results, agent_types, env_configs, log_dir)\n",
    "    \n",
    "    return all_results, log_dir\n",
    "\n",
    "\n",
    "# Define environment configurations\n",
    "env_configs = [\n",
    "    # Config 1 (Original)\n",
    "    {\n",
    "        'init_positions': [[2, 1]],\n",
    "        'target_pos': [0, 3],\n",
    "        'info_locations': [\n",
    "            {'position': [1, 1], 'info_type': 'X', 'collection_order': 0},\n",
    "            {'position': [3, 0], 'info_type': 'Y', 'collection_order': 1},\n",
    "            {'position': [3, 2], 'info_type': 'Z', 'collection_order': 2},\n",
    "        ],\n",
    "        'ditches': [(1, 0), (2, 0), (1, 2)],\n",
    "        'hazards': [(1, 3), (2, 3), (3, 1)]\n",
    "    },\n",
    "    # Config 2\n",
    "    {\n",
    "        'init_positions': [[0, 0]],\n",
    "        'target_pos': [3, 3],\n",
    "        'info_locations': [\n",
    "            {'position': [1, 2], 'info_type': 'X', 'collection_order': 0},\n",
    "            {'position': [2, 1], 'info_type': 'Y', 'collection_order': 1},\n",
    "            {'position': [3, 1], 'info_type': 'Z', 'collection_order': 2},\n",
    "        ],\n",
    "        'ditches': [(0, 1), (1, 3), (2, 2)],\n",
    "        'hazards': [(0, 3), (2, 0), (3, 2)]\n",
    "    },\n",
    "    # Config 3\n",
    "    {\n",
    "        'init_positions': [[3, 3]],\n",
    "        'target_pos': [0, 0],\n",
    "        'info_locations': [\n",
    "            {'position': [0, 1], 'info_type': 'X', 'collection_order': 0},\n",
    "            {'position': [1, 3], 'info_type': 'Y', 'collection_order': 1},\n",
    "            {'position': [2, 0], 'info_type': 'Z', 'collection_order': 2},\n",
    "        ],\n",
    "        'ditches': [(1, 1), (2, 3), (3, 0)],\n",
    "        'hazards': [(0, 2), (2, 2), (3, 1)]\n",
    "    },\n",
    "    # Config 4\n",
    "    {\n",
    "        'init_positions': [[1, 0]],\n",
    "        'target_pos': [2, 3],\n",
    "        'info_locations': [\n",
    "            {'position': [0, 2], 'info_type': 'X', 'collection_order': 0},\n",
    "            {'position': [2, 2], 'info_type': 'Y', 'collection_order': 1},\n",
    "            {'position': [3, 0], 'info_type': 'Z', 'collection_order': 2},\n",
    "        ],\n",
    "        'ditches': [(0, 0), (1, 1), (3, 3)],\n",
    "        'hazards': [(0, 3), (1, 3), (3, 2)]\n",
    "    },\n",
    "    # Config 5\n",
    "    {\n",
    "        'init_positions': [[0, 3]],\n",
    "        'target_pos': [3, 0],\n",
    "        'info_locations': [\n",
    "            {'position': [1, 1], 'info_type': 'X', 'collection_order': 0},\n",
    "            {'position': [2, 3], 'info_type': 'Y', 'collection_order': 1},\n",
    "            {'position': [3, 2], 'info_type': 'Z', 'collection_order': 2},\n",
    "        ],\n",
    "        'ditches': [(0, 1), (2, 1), (3, 3)],\n",
    "        'hazards': [(1, 0), (1, 3), (2, 0)]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define agent configurations\n",
    "agent_types = [\n",
    "    # {\n",
    "    #     \"name\": \"Baseline_Static\",\n",
    "    #     \"agent_class\": QLearningAgentFlat,\n",
    "    #     \"sparse_reward\": False,\n",
    "    #     \"reward_shaping\": False,\n",
    "    #     \"attention\": False,\n",
    "    #     \"hierarchical\": False,\n",
    "    #     \"change_priorities\": None  # No changes\n",
    "    # },\n",
    "\n",
    "    # {\n",
    "    #     \"name\": \"Baseline-Boost_Static\",\n",
    "    #     \"agent_class\": QLearningAgentFlat_Boosted,\n",
    "    #     \"sparse_reward\": False,\n",
    "    #     \"reward_shaping\": False,\n",
    "    #     \"attention\": False,\n",
    "    #     \"hierarchical\": False,\n",
    "    #     \"change_priorities\": None  # No changes\n",
    "    # },\n",
    "\n",
    "    # {\n",
    "    #     \"name\": \"CA-MIQ_Static (Ours)\",\n",
    "    #     \"agent_class\": QLearningAgentMaxInfoRL,\n",
    "    #     \"sparse_reward\": False,\n",
    "    #     \"reward_shaping\": False,\n",
    "    #     \"attention\": False,\n",
    "    #     \"hierarchical\": False,\n",
    "    #     \"change_priorities\": None  # No changes\n",
    "    # },\n",
    "\n",
    "    {\n",
    "        \"name\": \"Baseline_Dynamic\",\n",
    "        \"agent_class\": QLearningAgentFlat,\n",
    "        \"sparse_reward\": False,\n",
    "        \"reward_shaping\": False,\n",
    "        \"attention\": False,\n",
    "        \"hierarchical\": False,\n",
    "        \"change_priorities\": {\n",
    "            2000: {'X': 2, 'Y': 0, 'Z': 1},  # Change from X-Y-Z to Y-Z-X\n",
    "            #3500: {'X': 0, 'Y': 2, 'Z': 1},  # Change to Z-X-Y\n",
    "        }\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"name\": \"Baseline-Boost_Dynamic\",\n",
    "        \"agent_class\": QLearningAgentFlat_Boosted,\n",
    "        \"sparse_reward\": False,\n",
    "        \"reward_shaping\": False,\n",
    "        \"attention\": False,\n",
    "        \"hierarchical\": False,\n",
    "        \"change_priorities\": {\n",
    "            2000: {'X': 2, 'Y': 0, 'Z': 1},  # Change from X-Y-Z to Y-Z-X\n",
    "            # 3500: {'X': 0, 'Y': 2, 'Z': 1},  # Change to Z-X-Y\n",
    "        }\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"name\": \"CA-MIQ_Dynamic (Ours)\",\n",
    "        \"agent_class\": QLearningAgentMaxInfoRL,\n",
    "        \"sparse_reward\": False,\n",
    "        \"reward_shaping\": False,\n",
    "        \"attention\": False,\n",
    "        \"hierarchical\": False,\n",
    "        \"change_priorities\": {\n",
    "            2000: {'X': 2, 'Y': 0, 'Z': 1},  # Change from X-Y-Z to Y-Z-X\n",
    "            # 3500: {'X': 0, 'Y': 2, 'Z': 1},  # Change to Z-X-Y\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run experiments\n",
    "if __name__ == \"__main__\":\n",
    "    NUM_TRIALS = 10       # Number of trials with different environments\n",
    "    NUM_EPISODES = 5000   # Number of episodes per agent training run\n",
    "    \n",
    "    results, log_dir = run_multi_env_experiments(\n",
    "        env_configs=env_configs,\n",
    "        agent_types=agent_types,\n",
    "        num_trials=NUM_TRIALS,\n",
    "        num_episodes=NUM_EPISODES\n",
    "    )\n",
    "    \n",
    "    print(f\"Experiment completed. Results saved to {log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define environment configurations\n",
    "env_configs = [\n",
    "    # Config 1 (Original)\n",
    "    {\n",
    "        'init_positions': [[2, 1]],\n",
    "        'target_pos': [0, 3],\n",
    "        'info_locations': [\n",
    "            {'position': [1, 1], 'info_type': 'X', 'collection_order': 0},\n",
    "            {'position': [3, 0], 'info_type': 'Y', 'collection_order': 1},\n",
    "            {'position': [3, 2], 'info_type': 'Z', 'collection_order': 2},\n",
    "        ],\n",
    "        'ditches': [(1, 0), (2, 0), (1, 2)],\n",
    "        'hazards': [(1, 3), (2, 3), (3, 1)]\n",
    "    },\n",
    "    # Config 2\n",
    "    {\n",
    "        'init_positions': [[0, 0]],\n",
    "        'target_pos': [3, 3],\n",
    "        'info_locations': [\n",
    "            {'position': [1, 2], 'info_type': 'X', 'collection_order': 0},\n",
    "            {'position': [2, 1], 'info_type': 'Y', 'collection_order': 1},\n",
    "            {'position': [3, 1], 'info_type': 'Z', 'collection_order': 2},\n",
    "        ],\n",
    "        'ditches': [(0, 1), (1, 3), (2, 2)],\n",
    "        'hazards': [(0, 3), (2, 0), (3, 2)]\n",
    "    },\n",
    "    # Config 3\n",
    "    {\n",
    "        'init_positions': [[3, 3]],\n",
    "        'target_pos': [0, 0],\n",
    "        'info_locations': [\n",
    "            {'position': [0, 1], 'info_type': 'X', 'collection_order': 0},\n",
    "            {'position': [1, 3], 'info_type': 'Y', 'collection_order': 1},\n",
    "            {'position': [2, 0], 'info_type': 'Z', 'collection_order': 2},\n",
    "        ],\n",
    "        'ditches': [(1, 1), (2, 3), (3, 0)],\n",
    "        'hazards': [(0, 2), (2, 2), (3, 1)]\n",
    "    },\n",
    "    # Config 4\n",
    "    {\n",
    "        'init_positions': [[1, 0]],\n",
    "        'target_pos': [2, 3],\n",
    "        'info_locations': [\n",
    "            {'position': [0, 2], 'info_type': 'X', 'collection_order': 0},\n",
    "            {'position': [2, 2], 'info_type': 'Y', 'collection_order': 1},\n",
    "            {'position': [3, 0], 'info_type': 'Z', 'collection_order': 2},\n",
    "        ],\n",
    "        'ditches': [(0, 0), (1, 1), (3, 3)],\n",
    "        'hazards': [(0, 3), (1, 3), (3, 2)]\n",
    "    },\n",
    "    # Config 5\n",
    "    {\n",
    "        'init_positions': [[0, 3]],\n",
    "        'target_pos': [3, 0],\n",
    "        'info_locations': [\n",
    "            {'position': [1, 1], 'info_type': 'X', 'collection_order': 0},\n",
    "            {'position': [2, 3], 'info_type': 'Y', 'collection_order': 1},\n",
    "            {'position': [3, 2], 'info_type': 'Z', 'collection_order': 2},\n",
    "        ],\n",
    "        'ditches': [(0, 1), (2, 1), (3, 3)],\n",
    "        'hazards': [(1, 0), (1, 3), (2, 0)]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define agent configurations\n",
    "agent_types = [\n",
    "    {\n",
    "        \"name\": \"Baseline_Static\",\n",
    "        \"agent_class\": QLearningAgentFlat,\n",
    "        \"sparse_reward\": False,\n",
    "        \"reward_shaping\": False,\n",
    "        \"attention\": False,\n",
    "        \"hierarchical\": False,\n",
    "        \"change_priorities\": None  # No changes\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"name\": \"Baseline-Boost_Static\",\n",
    "        \"agent_class\": QLearningAgentFlat_Boosted,\n",
    "        \"sparse_reward\": False,\n",
    "        \"reward_shaping\": False,\n",
    "        \"attention\": False,\n",
    "        \"hierarchical\": False,\n",
    "        \"change_priorities\": None  # No changes\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"name\": \"CA-MIQ_Static (Ours)\",\n",
    "        \"agent_class\": QLearningAgentMaxInfoRL,\n",
    "        \"sparse_reward\": False,\n",
    "        \"reward_shaping\": False,\n",
    "        \"attention\": False,\n",
    "        \"hierarchical\": False,\n",
    "        \"change_priorities\": None  # No changes\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"name\": \"Baseline_Dynamic\",\n",
    "        \"agent_class\": QLearningAgentFlat,\n",
    "        \"sparse_reward\": False,\n",
    "        \"reward_shaping\": False,\n",
    "        \"attention\": False,\n",
    "        \"hierarchical\": False,\n",
    "        \"change_priorities\": {\n",
    "            1700: {'X': 2, 'Y': 0, 'Z': 1},  # Change from X-Y-Z to Y-Z-X\n",
    "            # 3500: {'X': 1, 'Y': 2, 'Z': 0},  # Change to Z-X-Y\n",
    "        }\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"name\": \"Baseline-Boost_Dynamic\",\n",
    "        \"agent_class\": QLearningAgentFlat_Boosted,\n",
    "        \"sparse_reward\": False,\n",
    "        \"reward_shaping\": False,\n",
    "        \"attention\": False,\n",
    "        \"hierarchical\": False,\n",
    "        \"change_priorities\": {\n",
    "            1700: {'X': 2, 'Y': 0, 'Z': 1},  # Change from X-Y-Z to Y-Z-X\n",
    "            # 3500: {'X': 1, 'Y': 2, 'Z': 0},  # Change to Z-X-Y\n",
    "        }\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"name\": \"CA-MIQ_Dynamic (Ours)\",\n",
    "        \"agent_class\": QLearningAgentMaxInfoRL,\n",
    "        \"sparse_reward\": False,\n",
    "        \"reward_shaping\": False,\n",
    "        \"attention\": False,\n",
    "        \"hierarchical\": False,\n",
    "        \"change_priorities\": {\n",
    "            1700: {'X': 2, 'Y': 0, 'Z': 1},  # Change from X-Y-Z to Y-Z-X\n",
    "            # 3500: {'X': 1, 'Y': 2, 'Z': 0},  # Change to Z-X-Y\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the results file   logs_new_single/logs/multi_env_comparison_parallel_20250502_194121/all_results.pkl\n",
    "log_dir = \"logs_new_single/logs/multi_env_comparison_parallel_20250502_194121\"\n",
    "results_file = os.path.join(log_dir, \"all_results.pkl\")\n",
    "\n",
    "# Load the pickle file\n",
    "with open(results_file, 'rb') as f:\n",
    "    results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_visualizations(all_results, agent_types, env_configs, log_dir):\n",
    "    \"\"\"Generate comprehensive visualizations summarizing the experiment results.\"\"\"\n",
    "    # Create visualization directory\n",
    "    vis_dir = os.path.join(log_dir, \"visualizations\")\n",
    "    os.makedirs(vis_dir, exist_ok=True)\n",
    "    \n",
    "    # Organize data by agent type\n",
    "    agent_data = {agent[\"name\"]: {\"rewards\": [], \"steps\": [], \"metrics\": []} for agent in agent_types}\n",
    "    \n",
    "    # Group results by environment config\n",
    "    env_results = {i: {\"agent_performance\": {}} for i in range(len(env_configs))}\n",
    "    \n",
    "    # Extract data from all trials\n",
    "    for trial, data in all_results.items():\n",
    "        config_idx = data[\"config_idx\"]\n",
    "        \n",
    "        for agent_name, agent_result in data[\"results\"].items():\n",
    "            # Store raw rewards and steps data\n",
    "            agent_data[agent_name][\"rewards\"].append(agent_result[\"rewards\"])\n",
    "            agent_data[agent_name][\"steps\"].append(agent_result[\"steps\"])\n",
    "            \n",
    "            # Store metrics\n",
    "            if \"metrics\" in agent_result:\n",
    "                agent_data[agent_name][\"metrics\"].append(agent_result[\"metrics\"])\n",
    "            \n",
    "            # Store performance by environment\n",
    "            if agent_name not in env_results[config_idx][\"agent_performance\"]:\n",
    "                env_results[config_idx][\"agent_performance\"][agent_name] = []\n",
    "            \n",
    "            # Use last 100 episodes as final performance measure\n",
    "            last_100_rewards = agent_result[\"rewards\"][-100:]\n",
    "            env_results[config_idx][\"agent_performance\"][agent_name].append(np.mean(last_100_rewards))\n",
    "    \n",
    "    # 1. Overall Agent Performance Summary (average across all environments)\n",
    "    print(\"\\nAgent Performance Summary (across all environments):\")\n",
    "    for agent_name, data in agent_data.items():\n",
    "        # Calculate average of last 100 episodes for each trial\n",
    "        final_performances = []\n",
    "        for reward_history in data[\"rewards\"]:\n",
    "            final_performances.append(np.mean(reward_history[-100:]))\n",
    "        \n",
    "        avg_performance = np.mean(final_performances)\n",
    "        std_performance = np.std(final_performances)\n",
    "        print(f\"Agent {agent_name}: {avg_performance:.2f} ± {std_performance:.2f}\")\n",
    "    \n",
    "    # 2. Create comparison chart for overall performance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot bars for each agent\n",
    "    agent_names = list(agent_data.keys())\n",
    "    x_pos = np.arange(len(agent_names))\n",
    "    \n",
    "    for i, agent_name in enumerate(agent_names):\n",
    "        # Calculate average of last 100 episodes for each trial\n",
    "        final_performances = []\n",
    "        for reward_history in agent_data[agent_name][\"rewards\"]:\n",
    "            final_performances.append(np.mean(reward_history[-100:]))\n",
    "        \n",
    "        plt.bar(x_pos[i], np.mean(final_performances), \n",
    "                yerr=np.std(final_performances), \n",
    "                capsize=10, \n",
    "                label=agent_name)\n",
    "    \n",
    "    plt.title(\"Average Agent Performance Across All Environment Configurations\")\n",
    "    plt.ylabel(\"Average Reward (Last 100 Episodes)\")\n",
    "    plt.xticks(x_pos, agent_names, rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(vis_dir, \"overall_agent_comparison.png\"))\n",
    "    \n",
    "    # 3. Static vs Dynamic Agent Comparison\n",
    "    window_size = 15  # For smoothing\n",
    "    \n",
    "    # 3.1 Static environment agents\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    for agent_name, data in agent_data.items():\n",
    "        if \"Static\" in agent_name:  # Only include static agents\n",
    "            # Average rewards across all trials\n",
    "            avg_rewards = np.mean(data[\"rewards\"], axis=0)\n",
    "            \n",
    "            # Smooth rewards using moving average\n",
    "            smoothed_rewards = np.convolve(avg_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "            plt.plot(smoothed_rewards, label=agent_name)\n",
    "            \n",
    "            # Calculate confidence intervals\n",
    "            all_smoothed = []\n",
    "            for reward_history in data[\"rewards\"]:\n",
    "                run_smoothed = np.convolve(reward_history, np.ones(window_size)/window_size, mode='valid')\n",
    "                all_smoothed.append(run_smoothed)\n",
    "            \n",
    "            all_smoothed = np.array(all_smoothed)\n",
    "            std_dev = np.std(all_smoothed, axis=0)\n",
    "            \n",
    "            # Plot confidence interval (±1 std dev)\n",
    "            x = np.arange(len(smoothed_rewards))\n",
    "            plt.fill_between(x, smoothed_rewards - std_dev, smoothed_rewards + std_dev, alpha=0.2)\n",
    "    \n",
    "    plt.title('Learning Curves (w/o Priority Shift)')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Average Reward (Smoothed)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(os.path.join(vis_dir, 'reward_trends_static.png'))\n",
    "    \n",
    "    # 3.2 Dynamic environment agents\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    dynamic_smoothed_rewards = {}  # Store for min/max calculation\n",
    "    \n",
    "    for agent_name, data in agent_data.items():\n",
    "        if \"Dynamic\" in agent_name:  # Only include dynamic agents\n",
    "            # Average rewards across all trials\n",
    "            avg_rewards = np.mean(data[\"rewards\"], axis=0)\n",
    "            \n",
    "            # Smooth rewards using moving average\n",
    "            smoothed_rewards = np.convolve(avg_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "            plt.plot(smoothed_rewards, label=agent_name)\n",
    "            dynamic_smoothed_rewards[agent_name] = smoothed_rewards\n",
    "            \n",
    "            # Calculate confidence intervals\n",
    "            all_smoothed = []\n",
    "            for reward_history in data[\"rewards\"]:\n",
    "                run_smoothed = np.convolve(reward_history, np.ones(window_size)/window_size, mode='valid')\n",
    "                all_smoothed.append(run_smoothed)\n",
    "            \n",
    "            all_smoothed = np.array(all_smoothed)\n",
    "            std_dev = np.std(all_smoothed, axis=0)\n",
    "            \n",
    "            # Plot confidence interval (±1 std dev)\n",
    "            x = np.arange(len(smoothed_rewards))\n",
    "            plt.fill_between(x, smoothed_rewards - std_dev, smoothed_rewards + std_dev, alpha=0.2)\n",
    "    \n",
    "    # Add priority change markers (if we have data for dynamic agents)\n",
    "    if dynamic_smoothed_rewards:\n",
    "        # Calculate global min/max for consistent text placement\n",
    "        all_rewards = np.concatenate(list(dynamic_smoothed_rewards.values()))\n",
    "        min_reward = np.min(all_rewards)\n",
    "        max_reward = np.max(all_rewards)\n",
    "        \n",
    "        # Get the change priority episodes from one of the dynamic agents\n",
    "        for agent in agent_types:\n",
    "            if agent[\"change_priorities\"] is not None:\n",
    "                for episode in agent[\"change_priorities\"].keys():\n",
    "                    if episode >= window_size//2:\n",
    "                        adjusted_episode = episode - window_size//2\n",
    "                        \n",
    "                        bullet_y = max_reward + (max_reward - min_reward) * 0.05  # Slightly above highest reward\n",
    "                        plt.plot(adjusted_episode, \n",
    "                                bullet_y, \n",
    "                                marker='o', \n",
    "                                markersize=10, \n",
    "                                color='red', \n",
    "                                label='Priority Change' if episode == list(agent[\"change_priorities\"].keys())[0] else \"\")\n",
    "                break  # Only need one agent's change points as they're the same\n",
    "    \n",
    "    plt.title('Learning Curves (with 1 Priority Shift)')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Average Reward (Smoothed)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(os.path.join(vis_dir, 'reward_trends_dynamic.png'))\n",
    "    \n",
    "    # 4. Performance by Environment Configuration\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Setup\n",
    "    env_labels = [f\"Config {i+1}\" for i in range(len(env_configs))]\n",
    "    agent_names = list(agent_data.keys())\n",
    "    x = np.arange(len(env_labels))\n",
    "    width = 0.2  # width of the bars\n",
    "    \n",
    "    # Plot bars for each agent grouped by environment\n",
    "    for i, agent_name in enumerate(agent_names):\n",
    "        # Collect performance across environments\n",
    "        env_perf = []\n",
    "        env_std = []\n",
    "        \n",
    "        for env_idx in range(len(env_configs)):\n",
    "            if agent_name in env_results[env_idx][\"agent_performance\"]:\n",
    "                perf_values = env_results[env_idx][\"agent_performance\"][agent_name]\n",
    "                if perf_values:\n",
    "                    env_perf.append(np.mean(perf_values))\n",
    "                    env_std.append(np.std(perf_values))\n",
    "                else:\n",
    "                    env_perf.append(0)\n",
    "                    env_std.append(0)\n",
    "            else:\n",
    "                env_perf.append(0)\n",
    "                env_std.append(0)\n",
    "        \n",
    "        # Calculate the offset for this agent's bars\n",
    "        offset = width * (i - len(agent_names)/2 + 0.5)\n",
    "        \n",
    "        # Plot with error bars\n",
    "        plt.bar(x + offset, env_perf, width, label=agent_name, yerr=env_std, capsize=5)\n",
    "    \n",
    "    plt.title('Agent Performance by Environment Configuration')\n",
    "    plt.xlabel('Environment')\n",
    "    plt.ylabel('Average Final Reward')\n",
    "    plt.xticks(x, env_labels)\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=len(agent_names))\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(vis_dir, 'performance_by_environment.png'))\n",
    "\n",
    "    \n",
    "    # Replace section 5 in generate_summary_visualizations with this enhanced adaptation metrics analysis\n",
    "\n",
    "    # 5. Enhanced Adaptation Metrics Analysis\n",
    "    # Collect adaptation metrics if available\n",
    "    adaptation_metrics = {agent_name: [] for agent_name in agent_names if \"Dynamic\" in agent_name}\n",
    "\n",
    "    print(\"\\nProcessing adaptation metrics for analysis...\")\n",
    "    for agent_name in adaptation_metrics.keys():\n",
    "        for metrics_list in agent_data[agent_name][\"metrics\"]:\n",
    "            # Check both possible fields where the priority changes might be stored\n",
    "            if \"priority_changes\" in metrics_list:\n",
    "                adaptation_metrics[agent_name].append(metrics_list[\"priority_changes\"])\n",
    "            elif \"all_priority_changes\" in metrics_list:\n",
    "                # Alternative field from our updated agent\n",
    "                processed_changes = []\n",
    "                for change in metrics_list[\"all_priority_changes\"]:\n",
    "                    # Convert raw change data to the expected format\n",
    "                    processed_change = {\n",
    "                        'episode': change.get('episode', 0),\n",
    "                        'completed': change.get('adaptation_completed', False),\n",
    "                        'success_rate_before': change.get('success_rate_before', 0),\n",
    "                    }\n",
    "                    \n",
    "                    if change.get('adaptation_completed', False):\n",
    "                        processed_change.update({\n",
    "                            'steps_to_adapt': change.get('steps_to_adapt', 0),\n",
    "                            'episodes_to_adapt': change.get('episodes_to_adapt', 0),\n",
    "                            'success_rate_after': change.get('success_rate_after', 0)\n",
    "                        })\n",
    "                    else:\n",
    "                        # For incomplete adaptations\n",
    "                        processed_change.update({\n",
    "                            'steps_without_recovery': change.get('steps_without_recovery', 0),\n",
    "                            'episodes_without_recovery': change.get('episodes_without_recovery', 0)\n",
    "                        })\n",
    "                    \n",
    "                    processed_changes.append(processed_change)\n",
    "                \n",
    "                adaptation_metrics[agent_name].append(processed_changes)\n",
    "\n",
    "    # If we have adaptation metrics, create enhanced visualizations\n",
    "    if any(metrics for metrics in adaptation_metrics.values()):\n",
    "        print(f\"Found adaptation metrics, creating visualizations...\")\n",
    "        \n",
    "        # Find max number of changes\n",
    "        max_changes = 0\n",
    "        for metrics_list in adaptation_metrics.values():\n",
    "            for run_metrics in metrics_list:\n",
    "                max_changes = max(max_changes, len(run_metrics))\n",
    "        \n",
    "        if max_changes > 0:\n",
    "            print(f\"Found {max_changes} priority changes to analyze\")\n",
    "            \n",
    "            # Setup\n",
    "            labels = [f\"Change {i+1}\" for i in range(max_changes)]\n",
    "            x = np.arange(len(labels))\n",
    "            width = 0.35 / len(adaptation_metrics)\n",
    "            \n",
    "            # 5.1 Episodes to Adapt Analysis\n",
    "            plt.figure(figsize=(14, 12))\n",
    "            plt.subplot(3, 1, 1)\n",
    "            \n",
    "            # Plot episodes to adapt for each agent\n",
    "            for i, (agent_name, metrics_list) in enumerate(adaptation_metrics.items()):\n",
    "                # Calculate average episodes to adapt for each change\n",
    "                avg_episodes = []\n",
    "                std_episodes = []\n",
    "                \n",
    "                for change_idx in range(max_changes):\n",
    "                    episode_values = []\n",
    "                    \n",
    "                    for run_metrics in metrics_list:\n",
    "                        if change_idx < len(run_metrics):\n",
    "                            change = run_metrics[change_idx]\n",
    "                            if change.get('completed', False) and 'episodes_to_adapt' in change:\n",
    "                                episode_values.append(change['episodes_to_adapt'])\n",
    "                    \n",
    "                    if episode_values:\n",
    "                        avg_episodes.append(np.mean(episode_values))\n",
    "                        std_episodes.append(np.std(episode_values))\n",
    "                    else:\n",
    "                        avg_episodes.append(0)\n",
    "                        std_episodes.append(0)\n",
    "                \n",
    "                # Plot with offset for each agent\n",
    "                offset = width * (i - len(adaptation_metrics)/2 + 0.5)\n",
    "                plt.bar(x + offset, avg_episodes, width, label=agent_name, yerr=std_episodes, capsize=5)\n",
    "            \n",
    "            plt.ylabel('Episodes to Adapt')\n",
    "            plt.title('Number of Episodes Required to Adapt After Priority Changes')\n",
    "            plt.xticks(x, labels)\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # 5.2 Steps to Adapt Analysis (original plot)\n",
    "            plt.subplot(3, 1, 2)\n",
    "            \n",
    "            # Plot steps to adapt for each agent\n",
    "            for i, (agent_name, metrics_list) in enumerate(adaptation_metrics.items()):\n",
    "                # Calculate average steps to adapt for each change\n",
    "                avg_steps = []\n",
    "                std_steps = []\n",
    "                \n",
    "                for change_idx in range(max_changes):\n",
    "                    steps_values = []\n",
    "                    \n",
    "                    for run_metrics in metrics_list:\n",
    "                        if change_idx < len(run_metrics):\n",
    "                            change = run_metrics[change_idx]\n",
    "                            if change.get('completed', False) and 'steps_to_adapt' in change:\n",
    "                                steps_values.append(change['steps_to_adapt'])\n",
    "                    \n",
    "                    if steps_values:\n",
    "                        avg_steps.append(np.mean(steps_values))\n",
    "                        std_steps.append(np.std(steps_values))\n",
    "                    else:\n",
    "                        avg_steps.append(0)\n",
    "                        std_steps.append(0)\n",
    "                \n",
    "                # Plot with offset for each agent\n",
    "                offset = width * (i - len(adaptation_metrics)/2 + 0.5)\n",
    "                plt.bar(x + offset, avg_steps, width, label=agent_name, yerr=std_steps, capsize=5)\n",
    "            \n",
    "            plt.ylabel('Steps to Adapt')\n",
    "            plt.title('Total Environment Steps Required to Adapt After Priority Changes')\n",
    "            plt.xticks(x, labels)\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # 5.3 Success Rate Improvement\n",
    "            plt.subplot(3, 1, 3)\n",
    "            \n",
    "            # Plot success rate improvement for each agent\n",
    "            for i, (agent_name, metrics_list) in enumerate(adaptation_metrics.items()):\n",
    "                # Calculate success rate improvement for each change\n",
    "                avg_improvement = []\n",
    "                std_improvement = []\n",
    "                \n",
    "                for change_idx in range(max_changes):\n",
    "                    improvement_values = []\n",
    "                    \n",
    "                    for run_metrics in metrics_list:\n",
    "                        if change_idx < len(run_metrics):\n",
    "                            change = run_metrics[change_idx]\n",
    "                            if change.get('completed', False) and 'success_rate_before' in change and 'success_rate_after' in change:\n",
    "                                improvement = change['success_rate_after'] - change['success_rate_before']\n",
    "                                improvement_values.append(improvement)\n",
    "                    \n",
    "                    if improvement_values:\n",
    "                        avg_improvement.append(np.mean(improvement_values))\n",
    "                        std_improvement.append(np.std(improvement_values))\n",
    "                    else:\n",
    "                        avg_improvement.append(0)\n",
    "                        std_improvement.append(0)\n",
    "                \n",
    "                # Plot with offset for each agent\n",
    "                offset = width * (i - len(adaptation_metrics)/2 + 0.5)\n",
    "                plt.bar(x + offset, avg_improvement, width, label=agent_name, yerr=std_improvement, capsize=5)\n",
    "            \n",
    "            plt.ylabel('Success Rate Improvement (%)')\n",
    "            plt.title('Performance Improvement After Adaptation')\n",
    "            plt.xticks(x, labels)\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(vis_dir, 'adaptation_metrics.png'))\n",
    "            \n",
    "            # 5.4 Adaptation Success Rate Analysis\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            # Create a matrix of adaptation success rates for each agent and change\n",
    "            for i, (agent_name, metrics_list) in enumerate(adaptation_metrics.items()):\n",
    "                # Calculate success rate for each change\n",
    "                success_rates = []\n",
    "                \n",
    "                for change_idx in range(max_changes):\n",
    "                    total_runs = 0\n",
    "                    success_count = 0\n",
    "                    \n",
    "                    for run_metrics in metrics_list:\n",
    "                        if change_idx < len(run_metrics):\n",
    "                            total_runs += 1\n",
    "                            change = run_metrics[change_idx]\n",
    "                            if change.get('completed', False):\n",
    "                                success_count += 1\n",
    "                    \n",
    "                    if total_runs > 0:\n",
    "                        success_rates.append((success_count / total_runs) * 100)\n",
    "                    else:\n",
    "                        success_rates.append(0)\n",
    "                \n",
    "                # Plot with offset for each agent\n",
    "                offset = width * (i - len(adaptation_metrics)/2 + 0.5)\n",
    "                plt.bar(x + offset, success_rates, width, label=agent_name)\n",
    "            \n",
    "            plt.ylabel('Adaptation Success Rate (%)')\n",
    "            plt.title('Percentage of Runs Where Agent Successfully Adapted')\n",
    "            plt.xticks(x, labels)\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3, axis='y')\n",
    "            plt.savefig(os.path.join(vis_dir, 'adaptation_success_rates.png'))\n",
    "            \n",
    "            # 5.5 Adaptation Time Comparison\n",
    "            plt.figure(figsize=(14, 10))\n",
    "            \n",
    "            # First subplot: Recovery time in episodes\n",
    "            plt.subplot(2, 1, 1)\n",
    "            \n",
    "            # For each agent, create a box plot of episodes to adapt for each change\n",
    "            boxplot_data = []\n",
    "            agent_colors = plt.cm.tab10(np.linspace(0, 1, len(adaptation_metrics)))\n",
    "            \n",
    "            for agent_idx, (agent_name, metrics_list) in enumerate(adaptation_metrics.items()):\n",
    "                for change_idx in range(max_changes):\n",
    "                    episode_values = []\n",
    "                    \n",
    "                    for run_metrics in metrics_list:\n",
    "                        if change_idx < len(run_metrics):\n",
    "                            change = run_metrics[change_idx]\n",
    "                            if change.get('completed', False) and 'episodes_to_adapt' in change:\n",
    "                                episode_values.append(change['episodes_to_adapt'])\n",
    "                    \n",
    "                    if episode_values:\n",
    "                        boxplot_data.append({\n",
    "                            'label': f\"{agent_name}\\nChange {change_idx+1}\",\n",
    "                            'data': episode_values,\n",
    "                            'color': agent_colors[agent_idx]\n",
    "                        })\n",
    "            \n",
    "            if boxplot_data:\n",
    "                # Create box plots\n",
    "                boxes = plt.boxplot([item['data'] for item in boxplot_data], \n",
    "                                labels=[item['label'] for item in boxplot_data],\n",
    "                                patch_artist=True,\n",
    "                                showfliers=False)  # Hide outliers for clarity\n",
    "                \n",
    "                # Color boxes by agent\n",
    "                for box, item in zip(boxes['boxes'], boxplot_data):\n",
    "                    box.set(facecolor=item['color'])\n",
    "            \n",
    "            plt.ylabel('Episodes to Adapt')\n",
    "            plt.title('Distribution of Adaptation Time Across Runs (Episodes)')\n",
    "            plt.grid(True, alpha=0.3, axis='y')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            \n",
    "            # Second subplot: Recovery time in steps\n",
    "            plt.subplot(2, 1, 2)\n",
    "            \n",
    "            # For each agent, create a box plot of steps to adapt for each change\n",
    "            boxplot_data = []\n",
    "            \n",
    "            for agent_idx, (agent_name, metrics_list) in enumerate(adaptation_metrics.items()):\n",
    "                for change_idx in range(max_changes):\n",
    "                    step_values = []\n",
    "                    \n",
    "                    for run_metrics in metrics_list:\n",
    "                        if change_idx < len(run_metrics):\n",
    "                            change = run_metrics[change_idx]\n",
    "                            if change.get('completed', False) and 'steps_to_adapt' in change:\n",
    "                                step_values.append(change['steps_to_adapt'])\n",
    "                    \n",
    "                    if step_values:\n",
    "                        boxplot_data.append({\n",
    "                            'label': f\"{agent_name}\\nChange {change_idx+1}\",\n",
    "                            'data': step_values,\n",
    "                            'color': agent_colors[agent_idx]\n",
    "                        })\n",
    "            \n",
    "            if boxplot_data:\n",
    "                # Create box plots\n",
    "                boxes = plt.boxplot([item['data'] for item in boxplot_data], \n",
    "                                labels=[item['label'] for item in boxplot_data],\n",
    "                                patch_artist=True,\n",
    "                                showfliers=False)  # Hide outliers for clarity\n",
    "                \n",
    "                # Color boxes by agent\n",
    "                for box, item in zip(boxes['boxes'], boxplot_data):\n",
    "                    box.set(facecolor=item['color'])\n",
    "            \n",
    "            plt.ylabel('Steps to Adapt')\n",
    "            plt.title('Distribution of Adaptation Time Across Runs (Steps)')\n",
    "            plt.grid(True, alpha=0.3, axis='y')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(vis_dir, 'adaptation_time_distributions.png'))\n",
    "            \n",
    "            # 5.6 Print Detailed Adaptation Summary Table\n",
    "            print(\"\\n\" + \"=\"*100)\n",
    "            print(\"ADAPTATION METRICS SUMMARY\")\n",
    "            print(\"=\"*100)\n",
    "            \n",
    "            header = f\"{'Agent':<20} | {'Change':<10} | {'Success Rate':<15} | {'Avg Episodes':<15} | {'Avg Steps':<15} | {'Improvement':<15}\"\n",
    "            print(header)\n",
    "            print(\"=\"*100)\n",
    "            \n",
    "            for agent_name, metrics_list in adaptation_metrics.items():\n",
    "                for change_idx in range(max_changes):\n",
    "                    # Calculate metrics for this change and agent\n",
    "                    success_count = 0\n",
    "                    total_runs = 0\n",
    "                    episode_values = []\n",
    "                    step_values = []\n",
    "                    improvement_values = []\n",
    "                    \n",
    "                    for run_metrics in metrics_list:\n",
    "                        if change_idx < len(run_metrics):\n",
    "                            total_runs += 1\n",
    "                            change = run_metrics[change_idx]\n",
    "                            \n",
    "                            if change.get('completed', False):\n",
    "                                success_count += 1\n",
    "                                \n",
    "                                if 'episodes_to_adapt' in change:\n",
    "                                    episode_values.append(change['episodes_to_adapt'])\n",
    "                                \n",
    "                                if 'steps_to_adapt' in change:\n",
    "                                    step_values.append(change['steps_to_adapt'])\n",
    "                                \n",
    "                                if 'success_rate_before' in change and 'success_rate_after' in change:\n",
    "                                    improvement = change['success_rate_after'] - change['success_rate_before']\n",
    "                                    improvement_values.append(improvement)\n",
    "                    \n",
    "                    # Calculate statistics\n",
    "                    if total_runs > 0:\n",
    "                        success_rate = f\"{(success_count / total_runs) * 100:.1f}%\"\n",
    "                    else:\n",
    "                        success_rate = \"N/A\"\n",
    "                    \n",
    "                    avg_episodes = f\"{np.mean(episode_values):.2f}\" if episode_values else \"N/A\"\n",
    "                    avg_steps = f\"{np.mean(step_values):.2f}\" if step_values else \"N/A\"\n",
    "                    avg_improvement = f\"{np.mean(improvement_values):.2f}%\" if improvement_values else \"N/A\"\n",
    "                    \n",
    "                    # Print row\n",
    "                    row = f\"{agent_name:<20} | {f'Change {change_idx+1}':<10} | {success_rate:<15} | {avg_episodes:<15} | {avg_steps:<15} | {avg_improvement:<15}\"\n",
    "                    print(row)\n",
    "            \n",
    "            # 5.7 Overall Adaptation Effectiveness Summary\n",
    "            print(\"\\n\" + \"=\"*100)\n",
    "            print(\"OVERALL ADAPTATION EFFECTIVENESS\")\n",
    "            print(\"=\"*100)\n",
    "            \n",
    "            for agent_name, metrics_list in adaptation_metrics.items():\n",
    "                # Calculate overall metrics across all changes\n",
    "                total_changes = 0\n",
    "                successful_adaptations = 0\n",
    "                total_episodes = []\n",
    "                total_steps = []\n",
    "                \n",
    "                for run_metrics in metrics_list:\n",
    "                    for change in run_metrics:\n",
    "                        total_changes += 1\n",
    "                        \n",
    "                        if change.get('completed', False):\n",
    "                            successful_adaptations += 1\n",
    "                            \n",
    "                            if 'episodes_to_adapt' in change:\n",
    "                                total_episodes.append(change['episodes_to_adapt'])\n",
    "                            \n",
    "                            if 'steps_to_adapt' in change:\n",
    "                                total_steps.append(change['steps_to_adapt'])\n",
    "                \n",
    "                # Calculate overall statistics\n",
    "                if total_changes > 0:\n",
    "                    overall_success_rate = (successful_adaptations / total_changes) * 100\n",
    "                    print(f\"\\nAgent: {agent_name}\")\n",
    "                    print(f\"  Total priority changes: {total_changes}\")\n",
    "                    print(f\"  Successfully adapted: {successful_adaptations} ({overall_success_rate:.1f}%)\")\n",
    "                    \n",
    "                    if successful_adaptations > 0:\n",
    "                        avg_episodes = np.mean(total_episodes) if total_episodes else \"N/A\"\n",
    "                        avg_steps = np.mean(total_steps) if total_steps else \"N/A\"\n",
    "                        print(f\"  Average episodes to adapt: {avg_episodes:.2f}\")\n",
    "                        print(f\"  Average steps to adapt: {avg_steps:.2f}\")\n",
    "                        \n",
    "                        # Calculate statistics by change type\n",
    "                        if max_changes > 1:\n",
    "                            print(\"\\n  Breakdown by change:\")\n",
    "                            for change_idx in range(max_changes):\n",
    "                                change_episodes = []\n",
    "                                change_steps = []\n",
    "                                change_success = 0\n",
    "                                change_total = 0\n",
    "                                \n",
    "                                for run_metrics in metrics_list:\n",
    "                                    if change_idx < len(run_metrics):\n",
    "                                        change_total += 1\n",
    "                                        change = run_metrics[change_idx]\n",
    "                                        \n",
    "                                        if change.get('completed', False):\n",
    "                                            change_success += 1\n",
    "                                            \n",
    "                                            if 'episodes_to_adapt' in change:\n",
    "                                                change_episodes.append(change['episodes_to_adapt'])\n",
    "                                            \n",
    "                                            if 'steps_to_adapt' in change:\n",
    "                                                change_steps.append(change['steps_to_adapt'])\n",
    "                                \n",
    "                                if change_total > 0:\n",
    "                                    change_success_rate = (change_success / change_total) * 100\n",
    "                                    avg_change_episodes = np.mean(change_episodes) if change_episodes else \"N/A\"\n",
    "                                    print(f\"    Change {change_idx+1}: {change_success_rate:.1f}% success rate, {avg_change_episodes:.2f} avg episodes\")\n",
    "    \n",
    "    # 6. Performance metrics comparison\n",
    "    # Define key metrics to compare\n",
    "    metrics_to_compare = [\n",
    "        'mission_success_rate', \n",
    "        'info_collection_success_rate',\n",
    "        'average_steps_per_episode',\n",
    "        'mission_success_no_collisions_rate'\n",
    "    ]\n",
    "    \n",
    "    metric_labels = {\n",
    "        'mission_success_rate': 'Mission Success (%)',\n",
    "        'info_collection_success_rate': 'Info Collection (%)',\n",
    "        'average_steps_per_episode': 'Avg Steps',\n",
    "        'mission_success_no_collisions_rate': 'Success Without Collisions (%)'\n",
    "    }\n",
    "    \n",
    "    # Check if we have these metrics\n",
    "    have_metrics = True\n",
    "    for agent_name, data in agent_data.items():\n",
    "        if not data[\"metrics\"] or not all(metric in data[\"metrics\"][0] for metric in metrics_to_compare):\n",
    "            have_metrics = False\n",
    "            break\n",
    "    \n",
    "    if have_metrics:\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        \n",
    "        # Setup\n",
    "        x = np.arange(len(metrics_to_compare))\n",
    "        width = 0.2  # width of the bars\n",
    "        \n",
    "        # Calculate average metrics for each agent\n",
    "        for i, agent_name in enumerate(agent_names):\n",
    "            # Collect all values for each metric\n",
    "            metric_values = []\n",
    "            metric_stds = []\n",
    "            \n",
    "            for metric in metrics_to_compare:\n",
    "                values = [m.get(metric, 0) for m in agent_data[agent_name][\"metrics\"]]\n",
    "                metric_values.append(np.mean(values))\n",
    "                metric_stds.append(np.std(values))\n",
    "            \n",
    "            # Calculate the offset for this agent's bars\n",
    "            offset = width * (i - len(agent_names)/2 + 0.5)\n",
    "            \n",
    "            # Plot with error bars\n",
    "            plt.bar(x + offset, metric_values, width, label=agent_name, yerr=metric_stds, capsize=5)\n",
    "        \n",
    "        plt.xlabel('Metrics')\n",
    "        plt.ylabel('Value')\n",
    "        plt.title('Performance Metrics Comparison')\n",
    "        plt.xticks(x, [metric_labels[metric] for metric in metrics_to_compare])\n",
    "        plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=len(agent_names))\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(vis_dir, 'performance_metrics.png'))\n",
    "        \n",
    "        # Print summary table\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"PERFORMANCE COMPARISON SUMMARY (AVERAGED ACROSS ALL ENVIRONMENTS)\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        header = f\"{'Metric':<40} | \" + \" | \".join([f\"{name:<15}\" for name in agent_names])\n",
    "        print(header)\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        metrics_to_print = metrics_to_compare + ['average_reward_per_episode']\n",
    "        \n",
    "        for metric in metrics_to_print:\n",
    "            values = []\n",
    "            for agent_name in agent_names:\n",
    "                # Calculate average value across all trials\n",
    "                metric_values = [m.get(metric, 0) for m in agent_data[agent_name][\"metrics\"] if metric in m]\n",
    "                if metric_values:\n",
    "                    avg_value = np.mean(metric_values)\n",
    "                    values.append(f\"{avg_value:.2f}\")\n",
    "                else:\n",
    "                    values.append(\"N/A\")\n",
    "            \n",
    "            metric_name = metric_labels.get(metric, metric)\n",
    "            row = f\"{metric_name:<40} | \" + \" | \".join([f\"{val:<15}\" for val in values])\n",
    "            print(row)\n",
    "    \n",
    "    print(f\"\\nResults and visualizations saved to {log_dir}\")\n",
    "    return vis_dir\n",
    "generate_summary_visualizations(results, agent_types, env_configs, log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import evaluate_trained_policy\n",
    "evaluate_trained_policy(agent, \"logs/comparison_20250428_173812/policies/MaxInfoRL_Dynamic/q_extrinsic_table_episode_5000.npy\")\n",
    "# evaluate_trained_policy(agent_maxinfo, \"policies/ADV/q_extrinsic_table_episode_5000.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR TESTING ### newwww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from environment_sar import SARrobotEnv\n",
    "from agents import QLearningAgentFlat, QLearningAgentMaxInfoRL\n",
    "from evaluation import plot_accumulated_rewards, load_training_results, save_training_results\n",
    "from robot_utils import RunningParameters\n",
    "param = RunningParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiments\n",
    "experiments = [\n",
    "    {\n",
    "        \"change_priorities\": {\n",
    "            1700: {'X': 2, 'Y': 0, 'Z': 1},  # Change from X-Y-Z to Y-Z-X\n",
    "            # 3500: {'X': 0, 'Y': 2, 'Z': 1},  # Change to Z-X-Y\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "agent_config = {\n",
    "    # List of agent variable names as strings\n",
    "    'agent_vars': [\n",
    "        'all_total_rewards_AGENT_flat',\n",
    "        'all_total_rewards_AGENT_flatB',\n",
    "        'all_total_rewards_AGENTmaxinfo'     \n",
    "        \n",
    "    ],\n",
    "    # Corresponding labels and colors for each agent\n",
    "    'labels': [\n",
    "        'Q-learning-Flat', \n",
    "        'Q-learning-Boost',\n",
    "        'Q-learning-Dual'\n",
    "    ],\n",
    "    'colors': [\n",
    "        'blue',\n",
    "        'orange',  \n",
    "        'green'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1) Q-learning-flat -- TESTING\n",
    "\n",
    "all_total_rewards_AGENT_flat = []  # List to store total rewards from each run\n",
    "all_total_steps_AGENT_flat = []  # List to store total rewards from each run\n",
    "all_metrics_AGENT_flat = [] # New list to store metrics from each run\n",
    "for exp in experiments:  # Loop through experiments\n",
    "    for _ in range(param.testing_runs):\n",
    "        env = SARrobotEnv(\n",
    "            grid_rows=4,\n",
    "            grid_cols=4,\n",
    "            info_number_needed=3,\n",
    "        )\n",
    "        \n",
    "        EPISODES = param.EPISODES\n",
    "        ALPHA = param.ALPHA\n",
    "        GAMMA = param.GAMMA\n",
    "        EPSILON_MAX = param.EPSILON_MAX\n",
    "        EPSILON_MIN = param.EPSILON_MIN\n",
    "        DECAY_RATE = param.DECAY_RATE\n",
    "        agent_flat = QLearningAgentFlat(env, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN,\n",
    "                                        log_rewards_dir=\"curves/flat\", learned_policy_dir=\"policies/flat\", boost=False)\n",
    "        rewards_flat, steps_flat, metrics_flat = agent_flat.train(5000, change_priorities_at=exp[\"change_priorities\"])\n",
    "\n",
    "        all_total_rewards_AGENT_flat.append(rewards_flat)\n",
    "        all_total_steps_AGENT_flat.append(steps_flat)\n",
    "        all_metrics_AGENT_flat.append(metrics_flat)  # Store the metrics\n",
    "\n",
    "    # save_training_results(agent_config['labels'][0], all_total_rewards_AGENT_flat, all_total_steps_AGENT_flat, all_metrics_AGENT_flat, save_dir='saved_results_no_sparse')\n",
    "# time.sleep(param.sleeping_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1) Q-learning-flat -- TESTING -- with BOOST\n",
    "all_total_rewards_AGENT_flatB = []  # List to store total rewards from each run\n",
    "all_total_steps_AGENT_flatB = []  # List to store total rewards from each run\n",
    "all_metrics_AGENT_flatB = [] # New list to store metrics from each run\n",
    "for exp in experiments:  # Loop through experiments\n",
    "    for _ in range(param.testing_runs):\n",
    "        env = SARrobotEnv(\n",
    "            grid_rows=4,\n",
    "            grid_cols=4,\n",
    "            info_number_needed=3,\n",
    "        )\n",
    "        \n",
    "        EPISODES = param.EPISODES\n",
    "        ALPHA = param.ALPHA\n",
    "        GAMMA = param.GAMMA\n",
    "        EPSILON_MAX = param.EPSILON_MAX\n",
    "        EPSILON_MIN = param.EPSILON_MIN\n",
    "        DECAY_RATE = param.DECAY_RATE\n",
    "        agent_flatB = QLearningAgentFlat(env, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN,\n",
    "                                        log_rewards_dir=\"curves/flatB\", learned_policy_dir=\"policies/flatB\", boost=True)\n",
    "        rewards_flatB, steps_flatB, metrics_flatB = agent_flatB.train(5000, change_priorities_at=exp[\"change_priorities\"])\n",
    "\n",
    "        all_total_rewards_AGENT_flatB.append(rewards_flatB)\n",
    "        all_total_steps_AGENT_flatB.append(steps_flatB)\n",
    "        all_metrics_AGENT_flatB.append(metrics_flatB)  # Store the metrics\n",
    "    # save_training_results(agent_config['labels'][1], all_total_rewards_AGENT_flatB, all_total_steps_AGENT_flatB, all_metrics_AGENT_flatB, save_dir='saved_results_no_sparse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_total_rewards_AGENTmaxinfo = []  # List to store total rewards from each run\n",
    "all_total_steps_AGENTmaxinfo = []  # List to store total rewards from each run\n",
    "all_metrics_AGENTmaxinfo = [] # New list to store metrics from each run\n",
    "for exp in experiments:  # Loop through experiments\n",
    "    for _ in range(param.testing_runs):\n",
    "        env = SARrobotEnv(\n",
    "            grid_rows=4,\n",
    "            grid_cols=4,\n",
    "            info_number_needed=3,\n",
    "        )\n",
    "        \n",
    "\n",
    "        EPISODES = param.EPISODES\n",
    "        ALPHA = param.ALPHA\n",
    "        GAMMA = param.GAMMA\n",
    "        EPSILON_MAX = param.EPSILON_MAX\n",
    "        EPSILON_MIN = param.EPSILON_MIN\n",
    "        DECAY_RATE = param.DECAY_RATE\n",
    "        agent_maxinfo = QLearningAgentMaxInfoRL(env, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN,\n",
    "                                        log_rewards_dir=\"curves/ADV\", learned_policy_dir=\"policies/ADV\", detect_shifts=False)\n",
    "        rewards_maxinfo, steps_maxinfo, metrics_maxinfo = agent_maxinfo.train(5000, change_priorities_at=exp[\"change_priorities\"])\n",
    "\n",
    "        all_total_rewards_AGENTmaxinfo.append(rewards_maxinfo)\n",
    "        all_total_steps_AGENTmaxinfo.append(steps_maxinfo)\n",
    "        all_metrics_AGENTmaxinfo.append(metrics_maxinfo)  # Store the metrics\n",
    "    # save_training_results(agent_config['labels'][2], all_total_rewards_AGENTmaxinfo, all_total_steps_AGENTmaxinfo, all_metrics_AGENTmaxinfo, save_dir='saved_results_no_sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store loaded rewards\n",
    "all_agent_rewards = {}\n",
    "\n",
    "# Load rewards for each agent from saved files\n",
    "for i, label in enumerate(agent_config['labels']):\n",
    "    # Load rewards using the existing function\n",
    "    loaded_data = load_training_results(\n",
    "        base_name=label,\n",
    "        data_type='rewards',  # Change from 'metrics' to 'rewards'\n",
    "        file_format='pickle',\n",
    "        save_dir='saved_results_no_sparse'\n",
    "    )\n",
    "    \n",
    "    # Add to dictionary if rewards were successfully loaded\n",
    "    if loaded_data is not None:\n",
    "        all_agent_rewards[label] = loaded_data\n",
    "    else:\n",
    "        print(f\"Warning: Could not load rewards for {label}\")\n",
    "\n",
    "# Call the plotting function with your data\n",
    "fig, ax = plot_accumulated_rewards(\n",
    "    reward_list=[all_agent_rewards[label] for label in agent_config['labels'] if label in all_agent_rewards],\n",
    "    labels=[label for label in agent_config['labels'] if label in all_agent_rewards],\n",
    "    colors=agent_config['colors'],\n",
    "    window_size=500,\n",
    "    figsize=(10, 6),\n",
    "    save_path='agent_performance_comparison',\n",
    "    use_savgol=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
