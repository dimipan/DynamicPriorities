{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment_sar import SARrobotEnv\n",
    "from agents import QLearningAgentFlat, QLearningAgentFlatLLM, QLearningAgentFlatAttention, QLearningAgentFlatActionToggle, LearningAgentFlat, QLearningAgentMaxInfoRL, QLearningAgentMaxInfoRL_ADVANCED\n",
    "from hierarchical_agents import QLearningAgentHierarchical, QLearningAgentHierarchicalLLM, QLearningAgentHierarchicalAttention, QLearningAgentHierarchicalActionToggle, LearningAgentHierarchical\n",
    "from robot_utils import RunningParameters, agent_config\n",
    "from evaluation import main_evaluation, compute_all_agents_metrics, evaluate_trained_policy, plot_accumulated_rewards, plot_accumulated_rewards_v2, save_training_results, load_training_results, plot_average_steps, plot_metric_bars\n",
    "import time\n",
    "param = RunningParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robot_utils import RunningParameters, agent_config\n",
    "param = RunningParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from environment_sar import SARrobotEnv\n",
    "from agents import QLearningAgentFlat, QLearningAgentMaxInfoRL_ADVANCED\n",
    "\n",
    "# Configuration\n",
    "GRID_ROWS = 4\n",
    "GRID_COLS = 4\n",
    "INFO_POINTS = 3  # Number of information points to collect\n",
    "NUM_EPISODES = 5000\n",
    "LOG_DIR = \"./logs/comparison_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "POLICY_DIR = os.path.join(LOG_DIR, \"policies\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "os.makedirs(POLICY_DIR, exist_ok=True)\n",
    "\n",
    "# RL parameters\n",
    "ALPHA = 0.1\n",
    "GAMMA = 0.99\n",
    "EPSILON_MAX = 1.0\n",
    "DECAY_RATE = 2\n",
    "EPSILON_MIN = 0.05\n",
    "\n",
    "# Define experiments\n",
    "experiments = [\n",
    "    {\n",
    "        \"name\": \"Flat_Static\",\n",
    "        \"agent_class\": QLearningAgentFlat,\n",
    "        \"sparse_reward\": False,\n",
    "        \"reward_shaping\": False,\n",
    "        \"attention\": False,\n",
    "        \"hierarchical\": False,\n",
    "        \"change_priorities\": None  # No changes\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Flat_Dynamic\",\n",
    "        \"agent_class\": QLearningAgentFlat,\n",
    "        \"sparse_reward\": False,\n",
    "        \"reward_shaping\": False,\n",
    "        \"attention\": False,\n",
    "        \"hierarchical\": False,\n",
    "        \"change_priorities\": {\n",
    "            2500: {'X': 2, 'Y': 0, 'Z': 1},  # Change from X-Y-Z to Y-Z-X\n",
    "            #3500: {'X': 1, 'Y': 2, 'Z': 0},   # Change to Z-X-Y\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MaxInfoRL_Static\",\n",
    "        \"agent_class\": QLearningAgentMaxInfoRL_ADVANCED,\n",
    "        \"sparse_reward\": False,\n",
    "        \"reward_shaping\": False,\n",
    "        \"attention\": False,\n",
    "        \"hierarchical\": False,\n",
    "        \"change_priorities\": None  # No changes\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MaxInfoRL_Dynamic\",\n",
    "        \"agent_class\": QLearningAgentMaxInfoRL_ADVANCED,\n",
    "        \"sparse_reward\": False,\n",
    "        \"reward_shaping\": False,\n",
    "        \"attention\": False,\n",
    "        \"hierarchical\": False,\n",
    "        \"change_priorities\": {\n",
    "            2500: {'X': 2, 'Y': 0, 'Z': 1},  # Change from X-Y-Z to Y-Z-X\n",
    "            #3500: {'X': 1, 'Y': 2, 'Z': 0},   # Change to Z-X-Y\n",
    "\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Run experiments\n",
    "for exp in experiments:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Running experiment: {exp['name']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create environment\n",
    "    env = SARrobotEnv(\n",
    "        grid_rows=GRID_ROWS,\n",
    "        grid_cols=GRID_COLS,\n",
    "        info_number_needed=INFO_POINTS,\n",
    "        sparse_reward=exp[\"sparse_reward\"],\n",
    "        reward_shaping=exp[\"reward_shaping\"],\n",
    "        attention=exp[\"attention\"],\n",
    "        hierarchical=exp[\"hierarchical\"],\n",
    "        render_mode=None\n",
    "    )\n",
    "    \n",
    "    # Create agent\n",
    "    agent = exp[\"agent_class\"](\n",
    "        env=env,\n",
    "        ALPHA=ALPHA,\n",
    "        GAMMA=GAMMA,\n",
    "        EPSILON_MAX=EPSILON_MAX,\n",
    "        DECAY_RATE=DECAY_RATE,\n",
    "        EPSILON_MIN=EPSILON_MIN,\n",
    "        log_rewards_dir=os.path.join(LOG_DIR, exp[\"name\"]),\n",
    "        learned_policy_dir=os.path.join(POLICY_DIR, exp[\"name\"])\n",
    "    )\n",
    "    \n",
    "    # Train agent\n",
    "    rewards, steps, metrics = agent.train(NUM_EPISODES, change_priorities_at=exp[\"change_priorities\"])\n",
    "    \n",
    "    # Store results\n",
    "    results[exp[\"name\"]] = {\n",
    "        \"rewards\": rewards,\n",
    "        \"steps\": steps,\n",
    "        \"metrics\": metrics,\n",
    "        \"agent_class\": exp[\"agent_class\"].__name__\n",
    "    }\n",
    "\n",
    "# Save results\n",
    "with open(os.path.join(LOG_DIR, 'results.pkl'), 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualizations\n",
    "# 1. Reward trends - Separate plots for static and dynamic\n",
    "window_size = 20  # For smoothing\n",
    "\n",
    "# Static environment plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "for exp_name, data in results.items():\n",
    "    if \"Static\" in exp_name:  # Only include static experiments\n",
    "        rewards = data[\"rewards\"]\n",
    "        # Smooth rewards using moving average\n",
    "        smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "        plt.plot(smoothed_rewards, label=exp_name)\n",
    "\n",
    "plt.title('Reward Trends During Training (Static Environment)')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward (Smoothed)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(LOG_DIR, 'reward_trends_static.png'))\n",
    "\n",
    "# Dynamic environment plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "dynamic_smoothed_rewards = {}  # Store for min/max calculation\n",
    "\n",
    "for exp_name, data in results.items():\n",
    "    if \"Dynamic\" in exp_name:  # Only include dynamic experiments\n",
    "        rewards = data[\"rewards\"]\n",
    "        # Smooth rewards using moving average\n",
    "        smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "        plt.plot(smoothed_rewards, label=exp_name)\n",
    "        dynamic_smoothed_rewards[exp_name] = smoothed_rewards\n",
    "\n",
    "# Add priority change markers (if we have data for dynamic experiments)\n",
    "if dynamic_smoothed_rewards:\n",
    "    # Calculate global min/max for consistent text placement\n",
    "    all_rewards = np.concatenate(list(dynamic_smoothed_rewards.values()))\n",
    "    min_reward = np.min(all_rewards)\n",
    "    max_reward = np.max(all_rewards)\n",
    "    \n",
    "    # Get the change priority episodes from one of the dynamic experiments\n",
    "    for exp in experiments:\n",
    "        if exp[\"change_priorities\"] is not None:\n",
    "            for episode in exp[\"change_priorities\"].keys():\n",
    "                if episode >= window_size//2:\n",
    "                    adjusted_episode = episode - window_size//2\n",
    "                    plt.axvline(x=adjusted_episode, color='r', linestyle='--', alpha=0.5)\n",
    "                    plt.text(adjusted_episode, min_reward + (max_reward-min_reward)*0.1, \n",
    "                            f\"Priority\\nChange\", rotation=90, color='r', alpha=0.7)\n",
    "            break  # Only need one experiment's change points as they're the same\n",
    "\n",
    "plt.title('Reward Trends During Training (Dynamic Environment)')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward (Smoothed)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(LOG_DIR, 'reward_trends_dynamic.png'))\n",
    "\n",
    "# 2. Adaptation metrics comparison (for dynamic experiments)\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Extract adaptation metrics\n",
    "flat_changes = []\n",
    "maxinfo_changes = []\n",
    "\n",
    "for exp_name, data in results.items():\n",
    "    if \"Dynamic\" in exp_name and \"priority_changes\" in data[\"metrics\"]:\n",
    "        changes = data[\"metrics\"][\"priority_changes\"]\n",
    "        if \"Flat\" in exp_name:\n",
    "            flat_changes = changes\n",
    "        else:\n",
    "            maxinfo_changes = changes\n",
    "\n",
    "# Compare adaptation metrics side by side\n",
    "if flat_changes and maxinfo_changes:\n",
    "    # Setup plot for adaptation time\n",
    "    plt.subplot(2, 1, 1)\n",
    "    labels = [f\"Change {i+1}\" for i in range(min(len(flat_changes), len(maxinfo_changes)))]\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Extract steps to adapt\n",
    "    flat_steps = [change.get('steps_to_adapt', 0) for change in flat_changes[:len(labels)]]\n",
    "    maxinfo_steps = [change.get('steps_to_adapt', 0) for change in maxinfo_changes[:len(labels)]]\n",
    "    \n",
    "    plt.bar(x - width/2, flat_steps, width, label='Flat Agent')\n",
    "    plt.bar(x + width/2, maxinfo_steps, width, label='MaxInfoRL Agent')\n",
    "    plt.ylabel('Steps to Adapt')\n",
    "    plt.title('Adaptation Time After Priority Changes')\n",
    "    plt.xticks(x, labels)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Setup plot for success rate improvement\n",
    "    plt.subplot(2, 1, 2)\n",
    "    \n",
    "    # Calculate success rate improvement (after - before)\n",
    "    flat_improvements = [change.get('success_rate_after', 0) - change.get('success_rate_before', 0) \n",
    "                        for change in flat_changes[:len(labels)]]\n",
    "    maxinfo_improvements = [change.get('success_rate_after', 0) - change.get('success_rate_before', 0) \n",
    "                           for change in maxinfo_changes[:len(labels)]]\n",
    "    \n",
    "    plt.bar(x - width/2, flat_improvements, width, label='Flat Agent')\n",
    "    plt.bar(x + width/2, maxinfo_improvements, width, label='MaxInfoRL Agent')\n",
    "    plt.ylabel('Success Rate Improvement (%)')\n",
    "    plt.title('Performance Improvement After Adaptation')\n",
    "    plt.xticks(x, labels)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(LOG_DIR, 'adaptation_metrics.png'))\n",
    "\n",
    "# 3. Overall performance comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Key metrics to compare\n",
    "metrics_to_compare = [\n",
    "    'mission_success_rate', \n",
    "    'info_collection_success_rate',\n",
    "    'average_steps_per_episode',\n",
    "    'mission_success_no_collisions_rate'\n",
    "]\n",
    "\n",
    "metric_labels = {\n",
    "    'mission_success_rate': 'Mission Success (%)',\n",
    "    'info_collection_success_rate': 'Info Collection (%)',\n",
    "    'average_steps_per_episode': 'Avg Steps',\n",
    "    'mission_success_no_collisions_rate': 'Success Without Collisions (%)'\n",
    "}\n",
    "\n",
    "# Setup bar chart\n",
    "x = np.arange(len(metrics_to_compare))\n",
    "width = 0.2\n",
    "exp_names = list(results.keys())\n",
    "\n",
    "for i, exp_name in enumerate(exp_names):\n",
    "    values = [results[exp_name]['metrics'].get(metric, 0) for metric in metrics_to_compare]\n",
    "    offset = width * (i - len(exp_names)/2 + 0.5)\n",
    "    plt.bar(x + offset, values, width, label=exp_name)\n",
    "\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Performance Comparison Across Experiments')\n",
    "plt.xticks(x, [metric_labels[metric] for metric in metrics_to_compare])\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=len(exp_names))\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(LOG_DIR, 'performance_comparison.png'))\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"PERFORMANCE COMPARISON SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Metric':<40} | {'Flat_Static':<15} | {'Flat_Dynamic':<15} | {'MaxInfoRL_Static':<15} | {'MaxInfoRL_Dynamic':<15}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "metrics_to_print = [\n",
    "    'mission_success_rate',\n",
    "    'info_collection_success_rate', \n",
    "    'mission_success_no_collisions_rate',\n",
    "    'average_steps_per_episode',\n",
    "    'average_reward_per_episode',\n",
    "    'exploration_exploitation_ratio'\n",
    "]\n",
    "\n",
    "for metric in metrics_to_print:\n",
    "    values = []\n",
    "    for exp_name in [\"Flat_Static\", \"Flat_Dynamic\", \"MaxInfoRL_Static\", \"MaxInfoRL_Dynamic\"]:\n",
    "        if exp_name in results:\n",
    "            value = results[exp_name][\"metrics\"].get(metric, \"N/A\")\n",
    "            if isinstance(value, (int, float)):\n",
    "                values.append(f\"{value:.2f}\")\n",
    "            else:\n",
    "                values.append(str(value))\n",
    "        else:\n",
    "            values.append(\"N/A\")\n",
    "    \n",
    "    metric_name = metric_labels.get(metric, metric)\n",
    "    print(f\"{metric_name:<40} | {values[0]:<15} | {values[1]:<15} | {values[2]:<15} | {values[3]:<15}\")\n",
    "\n",
    "# Print adaptation summary for dynamic experiments\n",
    "if flat_changes and maxinfo_changes:\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"ADAPTATION METRICS SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"{'Metric':<30} | {'Flat_Dynamic':<15} | {'MaxInfoRL_Dynamic':<15} | {'Improvement':<15}\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Average steps to adapt\n",
    "    flat_avg_steps = np.mean([change.get('steps_to_adapt', 0) for change in flat_changes if change.get('adaptation_completed', False)])\n",
    "    maxinfo_avg_steps = np.mean([change.get('steps_to_adapt', 0) for change in maxinfo_changes if change.get('adaptation_completed', False)])\n",
    "    step_diff = maxinfo_avg_steps - flat_avg_steps\n",
    "    step_pct = (flat_avg_steps - maxinfo_avg_steps) / flat_avg_steps * 100 if flat_avg_steps > 0 else 0\n",
    "    step_sign = \"+\" if step_pct > 0 else \"\"\n",
    "    \n",
    "    print(f\"{'Avg Steps to Adapt':<30} | {flat_avg_steps:.1f}:<15 | {maxinfo_avg_steps:.1f}:<15 | {step_sign}{step_pct:.1f}%\")\n",
    "    \n",
    "    # Average success rate improvement\n",
    "    flat_avg_improve = np.mean([change.get('success_rate_after', 0) - change.get('success_rate_before', 0) \n",
    "                               for change in flat_changes if change.get('adaptation_completed', False)])\n",
    "    maxinfo_avg_improve = np.mean([change.get('success_rate_after', 0) - change.get('success_rate_before', 0) \n",
    "                                  for change in maxinfo_changes if change.get('adaptation_completed', False)])\n",
    "    improve_diff = maxinfo_avg_improve - flat_avg_improve\n",
    "    improve_sign = \"+\" if improve_diff > 0 else \"\"\n",
    "    \n",
    "    print(f\"{'Avg Success Rate Improvement':<30} | {flat_avg_improve:.1f}%:<15 | {maxinfo_avg_improve:.1f}%:<15 | {improve_sign}{improve_diff:.1f}%\")\n",
    "\n",
    "print(\"\\nExperiment complete! Results saved to:\", LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import evaluate_trained_policy\n",
    "evaluate_trained_policy(agent, \"logs/comparison_20250428_134413/policies/MaxInfoRL_Dynamic/q_extrinsic_table_episode_5000.npy\")\n",
    "# evaluate_trained_policy(agent_maxinfo, \"policies/ADV/q_extrinsic_table_episode_5000.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR TESTING ### newwww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# List of models to test\n",
    "models = [\"tulu3:8b\", \"hermes3\", \"gemma2\", \"llama3.1\", \"qwen2.5\"]\n",
    "\n",
    "# Dictionary to store results for each model\n",
    "model_results = {}\n",
    "\n",
    "for model in models:\n",
    "    print(f\"\\n\\n==== Training with {model} model ====\\n\")\n",
    "\n",
    "    # Create environment once per model\n",
    "    env_hier = SARrobotEnv(\n",
    "        grid_rows=4,\n",
    "        grid_cols=4,\n",
    "        info_number_needed=6,\n",
    "        sparse_reward=True,\n",
    "        reward_shaping=False,\n",
    "        attention=False,\n",
    "        hierarchical=True,\n",
    "        render_mode='None'\n",
    "    )\n",
    "    \n",
    "    all_total_rewards_AGENT_hierLLM = []  # List to store total rewards from each run\n",
    "    all_total_steps_AGENT_hierLLM = []    # List to store total steps from each run\n",
    "    all_metrics_AGENT_hierLLM = []        # List to store metrics from each run\n",
    "    \n",
    "    for run in range(param.testing_runs):\n",
    "        print(f\"Starting run {run+1}/{param.testing_runs} with model {model}\")\n",
    "        \n",
    "        # Create manager with the current model\n",
    "        manager_hierLLM = QLearningAgentHierarchicalLLM(\n",
    "            env_hier, param.manager_action_space_size, \n",
    "            param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN, \n",
    "            log_rewards_dir=f\"curves/HIER-LLM-{model}\", \n",
    "            learned_policy_dir=f\"policies/HIER-LLM-{model}-manager\",\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        # Create workers with the current model\n",
    "        explore_worker_hierLLM = QLearningAgentHierarchicalLLM(\n",
    "            env_hier, param.explore_action_space_size, \n",
    "            param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN,\n",
    "            log_rewards_dir=None, \n",
    "            learned_policy_dir=f\"policies/HIER-LLM-{model}-explore\",\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        collect_worker_hierLLM = QLearningAgentHierarchicalLLM(\n",
    "            env_hier, param.collect_action_space_size, \n",
    "            param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN,\n",
    "            log_rewards_dir=None, \n",
    "            learned_policy_dir=f\"policies/HIER-LLM-{model}-collect\",\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        operate_worker_hierLLM = QLearningAgentHierarchicalLLM(\n",
    "            env_hier, param.operate_action_space_size, \n",
    "            param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN,\n",
    "            log_rewards_dir=None, \n",
    "            learned_policy_dir=f\"policies/HIER-LLM-{model}-operate\",\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        workers_hierLLM = {\n",
    "            0: explore_worker_hierLLM,  # Worker for EXPLORE\n",
    "            1: collect_worker_hierLLM,  # Worker for COLLECT\n",
    "            2: operate_worker_hierLLM   # Worker for OPERATE\n",
    "        }\n",
    "        \n",
    "        # Train the agent\n",
    "        rewards_hierLLM, steps_hierLLM, metrics_hierLLM, workers_LLM = manager_hierLLM.train(\n",
    "            manager_hierLLM, workers_hierLLM, param.EPISODES\n",
    "        )\n",
    "        \n",
    "        all_total_rewards_AGENT_hierLLM.append(rewards_hierLLM)\n",
    "        all_total_steps_AGENT_hierLLM.append(steps_hierLLM)\n",
    "        all_metrics_AGENT_hierLLM.append(metrics_hierLLM)\n",
    "    \n",
    "    # Save results for this model\n",
    "    save_training_results(\n",
    "        f\"{agent_config['labels'][6]}_{model}\", \n",
    "        all_total_rewards_AGENT_hierLLM, \n",
    "        all_total_steps_AGENT_hierLLM, \n",
    "        all_metrics_AGENT_hierLLM, \n",
    "        save_dir=f'saved_results_no_sparse/{model}'\n",
    "    )\n",
    "    \n",
    "    # Store results in dictionary for comparison\n",
    "    model_results[model] = {\n",
    "        'rewards': all_total_rewards_AGENT_hierLLM,\n",
    "        'steps': all_total_steps_AGENT_hierLLM,\n",
    "        'metrics': all_metrics_AGENT_hierLLM\n",
    "    }\n",
    "    \n",
    "    time.sleep(param.sleeping_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# List of models to evaluate\n",
    "models = [\"tulu3:8b\", \"hermes3\", \"gemma2\", \"llama3.1\", \"qwen2.5\"]\n",
    "\n",
    "# Dictionary to store loaded results for each model\n",
    "model_results = {}\n",
    "\n",
    "# Define which metrics to compute\n",
    "metric_keys = [\n",
    "    'mission_success_rate', \n",
    "    'info_collection_success_rate', \n",
    "    'collection_success_rate', \n",
    "    'average_steps_per_episode', \n",
    "    'average_reward_per_episode', \n",
    "    'collision_rate_in_successful_episodes',\n",
    "    'mission_success_no_collisions_rate', \n",
    "    'predictor_stats.overall_success_rate', \n",
    "    'llm_timing.average_time_per_call'\n",
    "]\n",
    "\n",
    "# Function to load saved results\n",
    "def load_saved_results(model, save_dir='saved_results_no_sparse'):\n",
    "    file_path = f\"{save_dir}/{model}/Q-learning-Hierarchical-LLM_{model}_metrics.pkl\"\n",
    "    \n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            return data\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading results for {model}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to get a nested dictionary value\n",
    "def get_nested_value(data, key_path):\n",
    "    \"\"\"Get a value from a nested dictionary using a dot-separated key path\"\"\"\n",
    "    keys = key_path.split('.')\n",
    "    current = data\n",
    "    \n",
    "    for key in keys:\n",
    "        if isinstance(current, dict) and key in current:\n",
    "            current = current[key]\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    return current\n",
    "\n",
    "# Function to process metrics and calculate averages\n",
    "def process_metrics_list(metrics_list):\n",
    "    \"\"\"Process a list of metrics dictionaries and compute averages for specified metrics\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    if not metrics_list or len(metrics_list) == 0:\n",
    "        return results\n",
    "    \n",
    "    # Process each metric key\n",
    "    for metric_key in metric_keys:\n",
    "        if '.' in metric_key:\n",
    "            # Handle nested metrics (e.g., predictor_stats.overall_success_rate)\n",
    "            values = [get_nested_value(m, metric_key) for m in metrics_list]\n",
    "            values = [v for v in values if v is not None]  # Filter out None values\n",
    "        else:\n",
    "            # Handle top-level metrics\n",
    "            values = [m.get(metric_key) for m in metrics_list if metric_key in m]\n",
    "        \n",
    "        # Calculate average if we have values\n",
    "        if values:\n",
    "            results[metric_key] = (np.mean(values), np.std(values))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load results for each model\n",
    "for model in models:\n",
    "    print(f\"Loading results for {model}...\")\n",
    "    data = load_saved_results(model)\n",
    "    if data is not None:\n",
    "        model_results[model] = data\n",
    "    else:\n",
    "        print(f\"Failed to load results for {model}\")\n",
    "\n",
    "# Process and print comparison summary\n",
    "print(\"\\n==== Model Comparison Summary ====\\n\")\n",
    "\n",
    "# Print each model's metrics\n",
    "for model in models:\n",
    "    if model in model_results:\n",
    "        print(f\"Model: {model}\")\n",
    "        processed_results = process_metrics_list(model_results[model])\n",
    "        \n",
    "        for metric_key in metric_keys:\n",
    "            if metric_key in processed_results:\n",
    "                mean, std = processed_results[metric_key]\n",
    "                \n",
    "                # Format the output based on metric type\n",
    "                if metric_key == 'llm_timing.average_time_per_call':\n",
    "                    print(f\" Average LLM call time: {mean:.4f} ± {std:.4f} seconds\")\n",
    "                elif metric_key == 'predictor_stats.overall_success_rate':\n",
    "                    print(f\" Average predictor success rate: {mean:.2f}% ± {std:.2f}%\")\n",
    "                elif metric_key == 'average_steps_per_episode':\n",
    "                    print(f\" Average steps: {mean:.2f} ± {std:.2f}\")\n",
    "                elif metric_key == 'average_reward_per_episode':\n",
    "                    print(f\" Average reward: {mean:.2f} ± {std:.2f}\")\n",
    "                elif metric_key == 'collection_success_rate':\n",
    "                    print(f\" Collection success rate: {mean:.2f}% ± {std:.2f}%\")\n",
    "                elif metric_key == 'collision_rate_in_successful_episodes':\n",
    "                    print(f\" Collision rate in successful episodes: {mean:.2f}% ± {std:.2f}%\")\n",
    "                elif metric_key == 'mission_success_no_collisions_rate':\n",
    "                    print(f\" Mission success without collisions rate: {mean:.2f}% ± {std:.2f}%\")\n",
    "                else:\n",
    "                    print(f\" {metric_key.replace('_', ' ').title()}: {mean:.2f}% ± {std:.2f}%\")\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR TESTING ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1) Q-learning-flat -- TESTING\n",
    "env = SARrobotEnv(\n",
    "            grid_rows=4,\n",
    "            grid_cols=4,\n",
    "            info_number_needed=3,\n",
    "            sparse_reward=False,\n",
    "            reward_shaping=False,\n",
    "            attention=False,\n",
    "            hierarchical=False,\n",
    "            render_mode='None'\n",
    "        )\n",
    "all_total_rewards_AGENT_flat = []  # List to store total rewards from each run\n",
    "all_total_steps_AGENT_flat = []  # List to store total rewards from each run\n",
    "all_metrics_AGENT_flat = [] # New list to store metrics from each run\n",
    "\n",
    "for _ in range(param.testing_runs):\n",
    "    EPISODES = param.EPISODES\n",
    "    ALPHA = param.ALPHA\n",
    "    GAMMA = param.GAMMA\n",
    "    EPSILON_MAX = param.EPSILON_MAX\n",
    "    EPSILON_MIN = 0.1\n",
    "    DECAY_RATE = param.DECAY_RATE\n",
    "    agent_flat = QLearningAgentFlat(env, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN,\n",
    "                                    log_rewards_dir=\"curves/flat\", learned_policy_dir=\"policies/flat\")\n",
    "    rewards_flat, steps_flat, metrics_flat = agent_flat.train(5000)\n",
    "\n",
    "    all_total_rewards_AGENT_flat.append(rewards_flat)\n",
    "    all_total_steps_AGENT_flat.append(steps_flat)\n",
    "    all_metrics_AGENT_flat.append(metrics_flat)  # Store the metrics\n",
    "# save_training_results(agent_config['labels'][0], all_total_rewards_AGENT_flat, all_total_steps_AGENT_flat, all_metrics_AGENT_flat, save_dir='saved_results_no_sparse')\n",
    "# time.sleep(param.sleeping_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SARrobotEnv(\n",
    "            grid_rows=4,\n",
    "            grid_cols=4,\n",
    "            info_number_needed=3,\n",
    "            sparse_reward=False,\n",
    "            reward_shaping=False,\n",
    "            attention=False,\n",
    "            hierarchical=False,\n",
    "            render_mode='None'\n",
    "        )\n",
    "all_total_rewards_AGENTmaxinfo = []  # List to store total rewards from each run\n",
    "all_total_steps_AGENTmaxinfo = []  # List to store total rewards from each run\n",
    "all_metrics_AGENTmaxinfo = [] # New list to store metrics from each run\n",
    "\n",
    "for _ in range(param.testing_runs):\n",
    "    EPISODES = param.EPISODES\n",
    "    ALPHA = param.ALPHA\n",
    "    GAMMA = param.GAMMA\n",
    "    EPSILON_MAX = param.EPSILON_MAX\n",
    "    EPSILON_MIN = param.EPSILON_MIN\n",
    "    DECAY_RATE = param.DECAY_RATE\n",
    "    agent_maxinfo = QLearningAgentMaxInfoRL(env, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN,\n",
    "                                    log_rewards_dir=\"curves/maxinfo\", learned_policy_dir=\"policies/maxinfo\")\n",
    "    rewards_maxinfo, steps_maxinfo, metrics_maxinfo = agent_maxinfo.train(1000)\n",
    "\n",
    "    all_total_rewards_AGENTmaxinfo.append(rewards_maxinfo)\n",
    "    all_total_steps_AGENTmaxinfo.append(steps_maxinfo)\n",
    "    all_metrics_AGENTmaxinfo.append(metrics_maxinfo)  # Store the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SARrobotEnv(\n",
    "            grid_rows=4,\n",
    "            grid_cols=4,\n",
    "            info_number_needed=3,\n",
    "            sparse_reward=False,\n",
    "            reward_shaping=False,\n",
    "            attention=False,\n",
    "            hierarchical=False,\n",
    "            render_mode='None'\n",
    "        )\n",
    "all_total_rewards_AGENTmaxinfo = []  # List to store total rewards from each run\n",
    "all_total_steps_AGENTmaxinfo = []  # List to store total rewards from each run\n",
    "all_metrics_AGENTmaxinfo = [] # New list to store metrics from each run\n",
    "\n",
    "for _ in range(param.testing_runs):\n",
    "    EPISODES = param.EPISODES\n",
    "    ALPHA = param.ALPHA\n",
    "    GAMMA = param.GAMMA\n",
    "    EPSILON_MAX = param.EPSILON_MAX\n",
    "    EPSILON_MIN = 0.1\n",
    "    DECAY_RATE = param.DECAY_RATE\n",
    "    agent_maxinfo = QLearningAgentMaxInfoRL_ADVANCED(env, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN,\n",
    "                                    log_rewards_dir=\"curves/ADV\", learned_policy_dir=\"policies/ADV\")\n",
    "    rewards_maxinfo, steps_maxinfo, metrics_maxinfo = agent_maxinfo.train(5000)\n",
    "\n",
    "    all_total_rewards_AGENTmaxinfo.append(rewards_maxinfo)\n",
    "    all_total_steps_AGENTmaxinfo.append(steps_maxinfo)\n",
    "    all_metrics_AGENTmaxinfo.append(metrics_maxinfo)  # Store the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## modular flat learning agent  -- TESTING\n",
    "\n",
    "env = SARrobotEnv(\n",
    "            grid_rows=4,\n",
    "            grid_cols=4,\n",
    "            info_number_needed=4,\n",
    "            sparse_reward=False,\n",
    "            reward_shaping=False,\n",
    "            attention=True,\n",
    "            hierarchical=False,\n",
    "            render_mode='None'\n",
    "        )\n",
    "all_total_rewards_AGENT = []  # List to store total rewards from each run\n",
    "all_total_steps_AGENT = []  # List to store total rewards from each run\n",
    "all_metrics_AGENT = [] # New list to store metrics from each run\n",
    "\n",
    "for _ in range(param.testing_runs):\n",
    "    EPISODES = param.EPISODES\n",
    "    ALPHA = param.ALPHA\n",
    "    GAMMA = param.GAMMA\n",
    "    EPSILON_MAX = param.EPSILON_MAX\n",
    "    EPSILON_MIN = param.EPSILON_MIN\n",
    "    DECAY_RATE = param.DECAY_RATE\n",
    "    agent = LearningAgentFlat(env, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN,\n",
    "                                    log_rewards_dir=\"curves/flatnewatt\", learned_policy_dir=\"policies/flatnewatt\",\n",
    "                                    use_llm=False, use_attention=True, use_action_toggle=False)\n",
    "    agent.global_epsilon_exploit = True\n",
    "    rewards, steps, metrics = agent.train(EPISODES)\n",
    "\n",
    "    all_total_rewards_AGENT.append(rewards)\n",
    "    all_total_steps_AGENT.append(steps)\n",
    "    all_metrics_AGENT.append(metrics)  # Store the metrics\n",
    "# save_training_results(agent_config['labels'][0], all_total_rewards_AGENT, all_total_steps_AGENT_flat, all_metrics_AGENT_flat, save_dir='saved_results_no_sparse')\n",
    "# time.sleep(param.sleeping_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## modular hierarchical learning agent  -- TESTING\n",
    "env_hier = SARrobotEnv(\n",
    "        grid_rows=4,\n",
    "        grid_cols=4,\n",
    "        info_number_needed=3,\n",
    "        sparse_reward=False,\n",
    "        reward_shaping=False,\n",
    "        attention=True,\n",
    "        hierarchical=True,\n",
    "        render_mode='None'\n",
    "        )\n",
    "all_total_rewards_AGENT_hier = []  # List to store total rewards from each rum\n",
    "all_total_steps_AGENT_hier = []  # List to store total rewards from each run\n",
    "all_metrics_AGENT_hier = [] # New list to store metrics from each run\n",
    "for _ in range(param.testing_runs):\n",
    "    manager_hier = LearningAgentHierarchical(env_hier, param.manager_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN, \n",
    "                                              log_rewards_dir=\"curves/HIER\", learned_policy_dir=\"policies/HIER-manager\",\n",
    "                                              use_llm=False, use_attention=False, use_action_toggle=True)\n",
    "    explore_worker_hier = LearningAgentHierarchical(env_hier, param.explore_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN,\n",
    "                                                     log_rewards_dir=None, learned_policy_dir=\"policies/HIER-explore\",\n",
    "                                                     use_llm=False, use_attention=False, use_action_toggle=True)\n",
    "    collect_worker_hier = LearningAgentHierarchical(env_hier, param.collect_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN,\n",
    "                                                     log_rewards_dir=None, learned_policy_dir=\"policies/HIER-collect\",\n",
    "                                                     use_llm=False, use_attention=False, use_action_toggle=True)\n",
    "    operate_worker_hier = LearningAgentHierarchical(env_hier, param.operate_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN,\n",
    "                                                     log_rewards_dir=None, learned_policy_dir=\"policies/HIER-operate\",\n",
    "                                                     use_llm=False, use_attention=False, use_action_toggle=True)\n",
    "    workers_hier = {\n",
    "        0: explore_worker_hier,  # Worker for EXPLORE\n",
    "        1: collect_worker_hier,  # Worker for COLLECT\n",
    "        2: operate_worker_hier   # Worker for OPERATE\n",
    "    }\n",
    "    manager_hier.global_epsilon_exploit = True\n",
    "    rewards_hier, steps_hier, metrics_hier, workers = manager_hier.train(manager_hier, workers_hier, param.EPISODES)\n",
    "    all_total_rewards_AGENT_hier.append(rewards_hier)\n",
    "    all_total_steps_AGENT_hier.append(steps_hier)\n",
    "    all_metrics_AGENT_hier.append(metrics_hier)  # Store the metrics\n",
    "# save_training_results(agent_config['labels'][5], all_total_rewards_AGENT_hier, all_total_steps_AGENT_hier, all_metrics_AGENT_hier, save_dir='saved_results_no_sparse')\n",
    "# time.sleep(param.sleeping_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2) Q-learning-LLM -- TESTING\n",
    "env = SARrobotEnv(\n",
    "            grid_rows=4,\n",
    "            grid_cols=4,\n",
    "            info_number_needed=3,\n",
    "            sparse_reward=False,\n",
    "            reward_shaping=False,\n",
    "            attention=False,\n",
    "            hierarchical=False,\n",
    "            render_mode='None'\n",
    "        )       \n",
    "all_total_rewards_AGENT_flatLLM = []  # List to store total rewards from each run\n",
    "all_total_steps_AGENT_flatLLM = []  # List to store total rewards from each run\n",
    "all_metrics_AGENT_flatLLM = [] # New list to store metrics from each run\n",
    "for _ in range(param.testing_runs): # changed\n",
    "    EPISODES = param.EPISODES # changed\n",
    "    ALPHA = param.ALPHA\n",
    "    GAMMA = param.GAMMA\n",
    "    EPSILON_MAX = param.EPSILON_MAX\n",
    "    EPSILON_MIN = param.EPSILON_MIN\n",
    "    DECAY_RATE = param.DECAY_RATE\n",
    "    agent_flatLLM = QLearningAgentFlatLLM(env, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN,\n",
    "                                          log_rewards_dir='curves/flat-LLM', learned_policy_dir='policies/flat-LLM')\n",
    "    rewards_flatLLM, steps_flatLLM, metrics_flatLLM = agent_flatLLM.train(EPISODES)\n",
    "\n",
    "    all_total_rewards_AGENT_flatLLM.append(rewards_flatLLM)\n",
    "    all_total_steps_AGENT_flatLLM.append(steps_flatLLM)\n",
    "    all_metrics_AGENT_flatLLM.append(metrics_flatLLM)  # Store the metrics\n",
    "save_training_results(agent_config['labels'][1], all_total_rewards_AGENT_flatLLM, all_total_steps_AGENT_flatLLM, all_metrics_AGENT_flatLLM, save_dir='saved_results_no_sparse')\n",
    "time.sleep(param.sleeping_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3) Q-learning-PolicyShaping -- TESTING\n",
    "env = SARrobotEnv(\n",
    "            grid_rows=4,\n",
    "            grid_cols=4,\n",
    "            info_number_needed=3,\n",
    "            sparse_reward=False,\n",
    "            reward_shaping=False,\n",
    "            attention=True,\n",
    "            hierarchical=False,\n",
    "            render_mode='None'\n",
    "        ) \n",
    "all_total_rewards_AGENT_att = []  # List to store total rewards from each run\n",
    "all_total_steps_AGENT_att = []  # List to store total rewards from each run\n",
    "all_metrics_AGENT_att = [] # New list to store metrics from each run\n",
    "for _ in range(param.testing_runs):\n",
    "    EPISODES = param.EPISODES\n",
    "    ALPHA = param.ALPHA\n",
    "    GAMMA = param.GAMMA\n",
    "    EPSILON_MAX = param.EPSILON_MAX\n",
    "    EPSILON_MIN = param.EPSILON_MIN\n",
    "    DECAY_RATE = param.DECAY_RATE\n",
    "    agent_att = QLearningAgentFlatAttention(env, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN,\n",
    "                                            log_rewards_dir=\"curves/flat-ATT-PS\", learned_policy_dir=\"policies/flat-ATT-PS\")\n",
    "    agent_att.global_epsilon_exploit = True\n",
    "    rewards_att, steps_att, metrics_att = agent_att.train(EPISODES)\n",
    "\n",
    "    all_total_rewards_AGENT_att.append(rewards_att)\n",
    "    all_total_steps_AGENT_att.append(steps_att)\n",
    "    all_metrics_AGENT_att.append(metrics_att)  # Store the metrics\n",
    "# save_training_results(agent_config['labels'][3], all_total_rewards_AGENT_att, all_total_steps_AGENT_att, all_metrics_AGENT_att, save_dir='saved_results_no_sparse')\n",
    "# time.sleep(param.sleeping_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4) Q-learning-RewardShaping -- TESTING\n",
    "env = SARrobotEnv(\n",
    "            grid_rows=4,\n",
    "            grid_cols=4,\n",
    "            info_number_needed=3,\n",
    "            sparse_reward=True,\n",
    "            reward_shaping=True,\n",
    "            attention=True,\n",
    "            hierarchical=False,\n",
    "            render_mode='None'\n",
    "        ) \n",
    "all_total_rewards_AGENT_attRS = []  # List to store total rewards from each run\n",
    "all_total_steps_AGENT_attRS = []  # List to store total rewards from each run\n",
    "all_metrics_AGENT_attRS = [] # New list to store metrics from each run\n",
    "for _ in range(param.testing_runs):\n",
    "    EPISODES = param.EPISODES\n",
    "    ALPHA = param.ALPHA\n",
    "    GAMMA = param.GAMMA\n",
    "    EPSILON_MAX = param.EPSILON_MAX\n",
    "    EPSILON_MIN = param.EPSILON_MIN\n",
    "    DECAY_RATE = param.DECAY_RATE\n",
    "    agent_attRS = QLearningAgentFlat(env, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN, \n",
    "                                            log_rewards_dir=\"curves/flat-ATT-RS\", learned_policy_dir=\"policies/flat-ATT-RS\")\n",
    "    rewards_attRS, steps_attRS, metrics_attRS = agent_attRS.train(EPISODES)\n",
    "\n",
    "    all_total_rewards_AGENT_attRS.append(rewards_attRS)\n",
    "    all_total_steps_AGENT_attRS.append(steps_attRS)\n",
    "    all_metrics_AGENT_attRS.append(metrics_attRS)  # Store the metrics\n",
    "save_training_results(agent_config['labels'][4], all_total_rewards_AGENT_attRS, all_total_steps_AGENT_attRS, all_metrics_AGENT_attRS, save_dir='saved_results_no_sparse')\n",
    "time.sleep(param.sleeping_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Q-learning-ActionToggle -- TESTING\n",
    "env = SARrobotEnv(\n",
    "            grid_rows=4,\n",
    "            grid_cols=4,\n",
    "            info_number_needed=3,\n",
    "            sparse_reward=True,\n",
    "            reward_shaping=False,\n",
    "            attention=True,\n",
    "            hierarchical=False,\n",
    "            render_mode='None'\n",
    "        ) \n",
    "all_total_rewards_AGENT_tog = []  # List to store total rewards from each run\n",
    "all_total_steps_AGENT_tog = []  # List to store total rewards from each run\n",
    "all_metrics_AGENT_tog = [] # New list to store metrics from each run\n",
    "for _ in range(param.testing_runs):\n",
    "    EPISODES = param.EPISODES\n",
    "    ALPHA = param.ALPHA\n",
    "    GAMMA = param.GAMMA\n",
    "    EPSILON_MAX = param.EPSILON_MAX\n",
    "    EPSILON_MIN = param.EPSILON_MIN\n",
    "    DECAY_RATE = param.DECAY_RATE\n",
    "    agent_tog = QLearningAgentFlatActionToggle(env, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN,\n",
    "                                            log_rewards_dir=\"curves/flat-ATT-AS\", learned_policy_dir=\"policies/flat-ATT-AS\") \n",
    "    rewards_tog, steps_tog, metrics_tog = agent_tog.train(EPISODES)\n",
    "\n",
    "    all_total_rewards_AGENT_tog.append(rewards_tog)\n",
    "    all_total_steps_AGENT_tog.append(steps_tog)\n",
    "    all_metrics_AGENT_tog.append(metrics_tog)  # Store the metrics\n",
    "save_training_results(agent_config['labels'][2], all_total_rewards_AGENT_tog, all_total_steps_AGENT_tog, all_metrics_AGENT_tog, save_dir='saved_results_no_sparse')\n",
    "time.sleep(param.sleeping_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Q-learning-hierarchical -- TESTING\n",
    "env_hier = SARrobotEnv(\n",
    "        grid_rows=4,\n",
    "        grid_cols=4,\n",
    "        info_number_needed=3,\n",
    "        sparse_reward=False,\n",
    "        reward_shaping=False,\n",
    "        attention=False,\n",
    "        hierarchical=True,\n",
    "        render_mode='None'\n",
    "        )\n",
    "all_total_rewards_AGENT_hier = []  # List to store total rewards from each rum\n",
    "all_total_steps_AGENT_hier = []  # List to store total rewards from each run\n",
    "all_metrics_AGENT_hier = [] # New list to store metrics from each run\n",
    "for _ in range(param.testing_runs):\n",
    "    manager_hier = QLearningAgentHierarchical(env_hier, param.manager_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN, \n",
    "                                              log_rewards_dir=\"curves/HIER\", learned_policy_dir=\"policies/HIER-manager\")\n",
    "    explore_worker_hier = QLearningAgentHierarchical(env_hier, param.explore_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN,\n",
    "                                                     log_rewards_dir=None, learned_policy_dir=\"policies/HIER-explore\")\n",
    "    collect_worker_hier = QLearningAgentHierarchical(env_hier, param.collect_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN,\n",
    "                                                     log_rewards_dir=None, learned_policy_dir=\"policies/HIER-collect\")\n",
    "    operate_worker_hier = QLearningAgentHierarchical(env_hier, param.operate_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN,\n",
    "                                                     log_rewards_dir=None, learned_policy_dir=\"policies/HIER-operate\")\n",
    "    workers_hier = {\n",
    "        0: explore_worker_hier,  # Worker for EXPLORE\n",
    "        1: collect_worker_hier,  # Worker for COLLECT\n",
    "        2: operate_worker_hier   # Worker for OPERATE\n",
    "    }\n",
    "    rewards_hier, steps_hier, metrics_hier, workers = manager_hier.train(manager_hier, workers_hier, param.EPISODES)\n",
    "    all_total_rewards_AGENT_hier.append(rewards_hier)\n",
    "    all_total_steps_AGENT_hier.append(steps_hier)\n",
    "    all_metrics_AGENT_hier.append(metrics_hier)  # Store the metrics\n",
    "save_training_results(agent_config['labels'][5], all_total_rewards_AGENT_hier, all_total_steps_AGENT_hier, all_metrics_AGENT_hier, save_dir='saved_results_no_sparse')\n",
    "time.sleep(param.sleeping_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Q-learning-hierarchical-LLM -- TESTING\n",
    "env_hier = SARrobotEnv(\n",
    "        grid_rows=4,\n",
    "        grid_cols=4,\n",
    "        info_number_needed=3,\n",
    "        sparse_reward=True,\n",
    "        reward_shaping=False,\n",
    "        attention=False,\n",
    "        hierarchical=True,\n",
    "        render_mode='None'\n",
    "        )\n",
    "all_total_rewards_AGENT_hierLLM = []  # List to store total rewards from each rum\n",
    "all_total_steps_AGENT_hierLLM = []  # List to store total rewards from each run\n",
    "all_metrics_AGENT_hierLLM = [] # New list to store metrics from each run\n",
    "for _ in range(param.testing_runs):\n",
    "    manager_hierLLM = QLearningAgentHierarchicalLLM(env_hier, param.manager_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN, \n",
    "                                                    log_rewards_dir=\"curves/HIER-LLM\", learned_policy_dir=\"policies/HIER-LLM-manager\")\n",
    "    explore_worker_hierLLM = QLearningAgentHierarchicalLLM(env_hier, param.explore_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN,\n",
    "                                                           log_rewards_dir=None, learned_policy_dir=\"policies/HIER-LLM-explore\")\n",
    "    collect_worker_hierLLM = QLearningAgentHierarchicalLLM(env_hier, param.collect_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN,\n",
    "                                                           log_rewards_dir=None, learned_policy_dir=\"policies/HIER-LLM-collect\")\n",
    "    operate_worker_hierLLM = QLearningAgentHierarchicalLLM(env_hier, param.operate_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN,\n",
    "                                                           log_rewards_dir=None, learned_policy_dir=\"policies/HIER-LLM-operate\")\n",
    "    workers_hierLLM = {\n",
    "        0: explore_worker_hierLLM,  # Worker for EXPLORE\n",
    "        1: collect_worker_hierLLM,  # Worker for COLLECT\n",
    "        2: operate_worker_hierLLM   # Worker for OPERATE\n",
    "    }\n",
    "    rewards_hierLLM, steps_hierLLM, metrics_hierLLM, workers_LLM = manager_hierLLM.train(manager_hierLLM, workers_hierLLM, param.EPISODES)\n",
    "    all_total_rewards_AGENT_hierLLM.append(rewards_hierLLM)\n",
    "    all_total_steps_AGENT_hierLLM.append(steps_hierLLM)\n",
    "    all_metrics_AGENT_hierLLM.append(metrics_hierLLM)  # Store the metrics\n",
    "save_training_results(agent_config['labels'][6], all_total_rewards_AGENT_hierLLM, all_total_steps_AGENT_hierLLM, all_metrics_AGENT_hierLLM, save_dir='saved_results_no_sparse')\n",
    "time.sleep(param.sleeping_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Q-learning-hierarchical-PolicyShaping -- TESTING\n",
    "all_total_rewards_AGENT_hier_att = []  # List to store total rewards from each run\n",
    "all_total_steps_AGENT_hier_att = []  # List to store total rewards from each run\n",
    "all_metrics_AGENT_hier_att = [] # New list to store metrics from each run\n",
    "for _ in range(param.testing_runs):\n",
    "    env_hier = SARrobotEnv(\n",
    "    grid_rows=4,\n",
    "    grid_cols=4,\n",
    "    info_number_needed=3,\n",
    "    sparse_reward=True,\n",
    "    reward_shaping=False,\n",
    "    attention=True,\n",
    "    hierarchical=True,\n",
    "    render_mode='None'\n",
    "    )\n",
    "    manager_hier_att = QLearningAgentHierarchicalAttention(env_hier, param.manager_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN, \n",
    "                                                           log_rewards_dir=\"curves/HIER-PS-manager\", learned_policy_dir=\"policies/HIER-PS-manager\")\n",
    "    explore_worker_hier_att = QLearningAgentHierarchicalAttention(env_hier, param.explore_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN,\n",
    "                                                                  log_rewards_dir=None, learned_policy_dir=\"policies/HIER-PS-explore\") \n",
    "    collect_worker_hier_att = QLearningAgentHierarchicalAttention(env_hier, param.collect_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN,\n",
    "                                                                  log_rewards_dir=None, learned_policy_dir=\"policies/HIER-PS-collect\") \n",
    "    operate_worker_hier_att = QLearningAgentHierarchicalAttention(env_hier, param.operate_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN,\n",
    "                                                                  log_rewards_dir=None, learned_policy_dir=\"policies/HIER-PS-operate\")\n",
    "    workers_hier_att = {\n",
    "        0: explore_worker_hier_att,  # Worker for EXPLORE\n",
    "        1: collect_worker_hier_att,  # Worker for COLLECT\n",
    "        2: operate_worker_hier_att   # Worker for OPERATE\n",
    "    }\n",
    "    manager_hier_att.global_epsilon_exploit = True\n",
    "    rewards_hier_att, steps_hier_att, metrics_hier_att, workers_att = manager_hier_att.train(manager_hier_att, workers_hier_att, param.EPISODES)\n",
    "    all_total_rewards_AGENT_hier_att.append(rewards_hier_att)\n",
    "    all_total_steps_AGENT_hier_att.append(steps_hier_att)\n",
    "    all_metrics_AGENT_hier_att.append(metrics_hier_att)  # Store the metrics\n",
    "save_training_results(agent_config['labels'][8], all_total_rewards_AGENT_hier_att, all_total_steps_AGENT_hier_att, all_metrics_AGENT_hier_att, save_dir='saved_results_no_sparse')\n",
    "time.sleep(param.sleeping_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9) Q-learning-hierarchical-RewardShaping -- TESTING\n",
    "env_hier = SARrobotEnv(\n",
    "        grid_rows=4,\n",
    "        grid_cols=4,\n",
    "        info_number_needed=3,\n",
    "        sparse_reward=True,\n",
    "        reward_shaping=True,\n",
    "        attention=True,\n",
    "        hierarchical=True,\n",
    "        render_mode='None'\n",
    "        )\n",
    "all_total_rewards_AGENT_hierRS = []  # List to store total rewards from each rum\n",
    "all_total_steps_AGENT_hierRS = []  # List to store total rewards from each run\n",
    "all_metrics_AGENT_hierRS = [] # New list to store metrics from each run\n",
    "for _ in range(param.testing_runs):\n",
    "    manager_hierRS = QLearningAgentHierarchical(env_hier, param.manager_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN, \n",
    "                                              log_rewards_dir=\"curves/HIER-RS\", learned_policy_dir=\"policies/HIER-manager-RS\")\n",
    "    explore_worker_hierRS = QLearningAgentHierarchical(env_hier, param.explore_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN,\n",
    "                                                     log_rewards_dir=None, learned_policy_dir=\"policies/HIER-explore-RS\")\n",
    "    collect_worker_hierRS = QLearningAgentHierarchical(env_hier, param.collect_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN,\n",
    "                                                     log_rewards_dir=None, learned_policy_dir=\"policies/HIER-collect-RS\")\n",
    "    operate_worker_hierRS = QLearningAgentHierarchical(env_hier, param.operate_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN,\n",
    "                                                     log_rewards_dir=None, learned_policy_dir=\"policies/HIER-operate-RS\")\n",
    "    workers_hierRS = {\n",
    "        0: explore_worker_hierRS,  # Worker for EXPLORE\n",
    "        1: collect_worker_hierRS,  # Worker for COLLECT\n",
    "        2: operate_worker_hierRS   # Worker for OPERATE\n",
    "    }\n",
    "    rewards_hier_attRS, steps_hier_attRS, metrics_hier_attRS, workers_hier_attRS = manager_hierRS.train(manager_hierRS, workers_hierRS, param.EPISODES)\n",
    "    all_total_rewards_AGENT_hierRS.append(rewards_hier_attRS)\n",
    "    all_total_steps_AGENT_hierRS.append(steps_hier_attRS)\n",
    "    all_metrics_AGENT_hierRS.append(metrics_hier_attRS)  # Store the metrics\n",
    "save_training_results(agent_config['labels'][9], all_total_rewards_AGENT_hierRS, all_total_steps_AGENT_hierRS, all_metrics_AGENT_hierRS, save_dir='saved_results_no_sparse')\n",
    "time.sleep(param.sleeping_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Q-learning-hierarchical-ActionToggle -- TESTING\n",
    "all_total_rewards_AGENT_hier_tog = []  # List to store total rewards from each run\n",
    "all_total_steps_AGENT_hier_tog = []  # List to store total rewards from each run\n",
    "all_metrics_AGENT_hier_tog = [] # New list to store metrics from each run\n",
    "for _ in range(param.testing_runs):\n",
    "    env_hier = SARrobotEnv(\n",
    "    grid_rows=4,\n",
    "    grid_cols=4,\n",
    "    info_number_needed=3,\n",
    "    sparse_reward=True,\n",
    "    reward_shaping=False,\n",
    "    attention=True,\n",
    "    hierarchical=True,\n",
    "    render_mode='None'\n",
    "    )\n",
    "    # Manager for choosing options\n",
    "    manager_hier_tog = QLearningAgentHierarchicalActionToggle(env_hier, param.manager_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN,\n",
    "                                                              log_rewards_dir=\"curves/HIER-AS\", learned_policy_dir=\"policies/HIER-manager-AS\")\n",
    "    explore_worker_hier_tog = QLearningAgentHierarchicalActionToggle(env_hier, param.explore_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN,\n",
    "                                                                     log_rewards_dir=None, learned_policy_dir=\"policies/HIER-explore-AS\") \n",
    "    collect_worker_hier_tog = QLearningAgentHierarchicalActionToggle(env_hier, param.collect_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN,\n",
    "                                                                     log_rewards_dir=None, learned_policy_dir=\"policies/HIER-collect-AS\")\n",
    "    operate_worker_hier_tog = QLearningAgentHierarchicalActionToggle(env_hier, param.operate_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN,\n",
    "                                                                     log_rewards_dir=None, learned_policy_dir=\"policies/HIER-operate-AS\")\n",
    "    workers_hier_tog = {\n",
    "        0: explore_worker_hier_tog,  # Worker for EXPLORE\n",
    "        1: collect_worker_hier_tog,  # Worker for COLLECT\n",
    "        2: operate_worker_hier_tog   # Worker for OPERATE\n",
    "    }\n",
    "    rewards_hier_tog, steps_hier_tog, metrics_hier_tog, workers_hier_tog = manager_hier_tog.train(manager_hier_tog, workers_hier_tog, param.EPISODES)\n",
    "    all_total_rewards_AGENT_hier_tog.append(rewards_hier_tog)\n",
    "    all_total_steps_AGENT_hier_tog.append(steps_hier_tog)\n",
    "    all_metrics_AGENT_hier_tog.append(metrics_hier_tog)  # Store the metrics\n",
    "save_training_results(agent_config['labels'][7], all_total_rewards_AGENT_hier_tog, all_total_steps_AGENT_hier_tog, all_metrics_AGENT_hier_tog, save_dir='saved_results_no_sparse')\n",
    "time.sleep(param.sleeping_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store loaded metrics\n",
    "all_agent_metrics = {}\n",
    "\n",
    "# Load metrics for each agent from saved files\n",
    "for i, label in enumerate(agent_config['labels']):\n",
    "    # Load metrics using the existing function\n",
    "    loaded_data = load_training_results(\n",
    "        base_name=label,\n",
    "        data_type='metrics',\n",
    "        file_format='pickle',\n",
    "        save_dir='saved_results_no_sparse'\n",
    "    )\n",
    "    \n",
    "    # Add to dictionary if metrics were successfully loaded\n",
    "    if loaded_data is not None:\n",
    "        all_agent_metrics[label]= loaded_data\n",
    "    else:\n",
    "        print(f\"Warning: Could not load metrics for {label}\")\n",
    "\n",
    "# Define which metrics to compute\n",
    "metric_keys = ['mission_success_rate', 'info_collection_success_rate', 'collection_success_rate', 'average_steps_per_episode', 'average_reward_per_episode', 'collision_rate_in_successful_episodes',\n",
    "               'mission_success_no_collisions_rate', 'predictor_stats.overall_success_rate', 'llm_timing.average_time_per_call']\n",
    "\n",
    "# Compute metrics for all available agents\n",
    "results = compute_all_agents_metrics(all_agent_metrics, metric_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_trained_policy(manager_hierLLM, \"policies/HIER-LLM-manager/manager_q_table_episode_1000.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store loaded rewards\n",
    "all_agent_rewards = {}\n",
    "\n",
    "# Load rewards for each agent from saved files\n",
    "for i, label in enumerate(agent_config['labels']):\n",
    "    # Load rewards using the existing function\n",
    "    loaded_data = load_training_results(\n",
    "        base_name=label,\n",
    "        data_type='rewards',  # Change from 'metrics' to 'rewards'\n",
    "        file_format='pickle',\n",
    "        save_dir='saved_results_no_sparse'\n",
    "    )\n",
    "    \n",
    "    # Add to dictionary if rewards were successfully loaded\n",
    "    if loaded_data is not None:\n",
    "        all_agent_rewards[label] = loaded_data\n",
    "    else:\n",
    "        print(f\"Warning: Could not load rewards for {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the plotting function with your data\n",
    "fig, ax = plot_accumulated_rewards(\n",
    "    reward_list=[all_agent_rewards[label] for label in agent_config['labels'] if label in all_agent_rewards],\n",
    "    labels=[label for label in agent_config['labels'] if label in all_agent_rewards],\n",
    "    colors=agent_config['colors'],\n",
    "    window_size=150,\n",
    "    figsize=(10, 6),\n",
    "    save_path='agent_performance_comparison',\n",
    "    use_savgol=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_bars(all_agent_metrics, agent_config, metric_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR TESTING ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7) Q-PS-RS (Q with attention mechanism - policy shaping + reward shaping)\n",
    "env = SARrobotEnv(\n",
    "            grid_rows=4,\n",
    "            grid_cols=4,\n",
    "            info_number_needed=3,\n",
    "            sparse_reward=True,\n",
    "            reward_shaping=True,\n",
    "            attention=True,\n",
    "            hierarchical=False,\n",
    "            render_mode='None'\n",
    "        ) \n",
    "all_total_rewards_AGENT_attPSRS = []  # List to store total rewards from each run\n",
    "all_total_steps_AGENT_attPSRS = []  # List to store total rewards from each run\n",
    "for _ in range(param.testing_runs):\n",
    "    EPISODES = param.EPISODES\n",
    "    ALPHA = param.ALPHA\n",
    "    GAMMA = param.GAMMA\n",
    "    EPSILON_MAX = param.EPSILON_MAX\n",
    "    EPSILON_MIN = param.EPSILON_MIN\n",
    "    DECAY_RATE = param.DECAY_RATE\n",
    "    agent_attPSRS = QLearningAgentFlatAttention(env, ALPHA, GAMMA, EPSILON_MAX, DECAY_RATE, EPSILON_MIN, \n",
    "                                            log_rewards_dir=\"curves-sparse/ATT-PSRS\", learned_policy_dir=\"policies-sparse/ATT-PSRS\")\n",
    "    returns_attPSRS, steps_attPSRS = agent_attPSRS.train(EPISODES)\n",
    "\n",
    "    all_total_rewards_AGENT_attPSRS.append(returns_attPSRS)\n",
    "    all_total_steps_AGENT_attPSRS.append(steps_attPSRS)\n",
    "\n",
    "time.sleep(param.sleeping_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8) HierQ-PS-RS (hierarchical Q-learning with attention mechanism - policy shaping + reward shaping)\n",
    "all_total_rewards_AGENT_hier_attPSRS = []  # List to store total rewards from each run\n",
    "for _ in range(param.testing_runs):\n",
    "    env_hier = SARrobotEnv(\n",
    "    grid_rows=4,\n",
    "    grid_cols=4,\n",
    "    info_number_needed=3,\n",
    "    sparse_reward=True,\n",
    "    reward_shaping=True,\n",
    "    attention=True,\n",
    "    hierarchical=True,\n",
    "    render_mode='None'\n",
    "    )\n",
    "    # Manager for choosing options\n",
    "    manager_hier_attPSRS = QLearningAgentHierarchicalAttention(env_hier, param.manager_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN, \n",
    "                                                           log_rewards_dir=\"curves-sparse/HRL-PS-RS-manager\", learned_policy_dir=\"policies-sparse/HRL-PS-RS-manager\")\n",
    "    explore_worker_hier_attPSRS = QLearningAgentHierarchicalAttention(env_hier, param.explore_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN,\n",
    "                                                                  log_rewards_dir=None, learned_policy_dir=\"policies-sparse/HRL-PS-RS-explore\") \n",
    "    collect_worker_hier_attPSRS = QLearningAgentHierarchicalAttention(env_hier, param.collect_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN,\n",
    "                                                                  log_rewards_dir=None, learned_policy_dir=\"policies-sparse/HRL-PS-RS-collect\") \n",
    "    operate_worker_hier_attPSRS = QLearningAgentHierarchicalAttention(env_hier, param.operate_action_space_size, param.ALPHA, param.GAMMA, param.EPSILON_MAX, param.DECAY_RATE, param.EPSILON_MIN,\n",
    "                                                                  log_rewards_dir=None, learned_policy_dir=\"policies-sparse/HRL-PS-RS-operate\")\n",
    "    workers_hier_attPSRS = {\n",
    "        0: explore_worker_hier_attPSRS,  # Worker for EXPLORE\n",
    "        1: collect_worker_hier_attPSRS,  # Worker for COLLECT\n",
    "        2: operate_worker_hier_attPSRS   # Worker for OPERATE\n",
    "    }\n",
    "    hier_returns_attPSRS, attentionPSRS, workers_simple_attPSRS = manager_hier_attPSRS.train(manager_hier_attPSRS, workers_hier_attPSRS, param.EPISODES)\n",
    "    all_total_rewards_AGENT_hier_attPSRS.append(hier_returns_attPSRS)\n",
    "\n",
    "time.sleep(param.sleeping_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SARrobotEnv(\n",
    "        grid_rows=4,\n",
    "        grid_cols=4,\n",
    "        info_number_needed=3,\n",
    "        sparse_reward=True,\n",
    "        reward_shaping=False,\n",
    "        attention=False,\n",
    "        hierarchical=False,\n",
    "        render_mode='None'\n",
    "    )\n",
    "all_total_rewards_DDQN = []  # List to store rewards from each trial\n",
    "for _ in range(param.testing_runs):\n",
    "    agent = DoubleDQNAgent(\n",
    "        env,\n",
    "        hidden_dims=[64, 64],\n",
    "        learning_rate=0.001,\n",
    "        gamma=0.99,\n",
    "        buffer_size=5000,\n",
    "        batch_size=256,\n",
    "        target_update=100,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=0.995\n",
    "    )\n",
    "    \n",
    "    agent.train(num_episodes=param.EPISODES, minimal_size=1000, save_interval=100)\n",
    "    all_total_rewards_DDQN.append(agent.episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from environment_sar import SARrobotEnv\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class TqdmProgressCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Custom callback for monitoring training progress with tqdm and displaying rewards.\n",
    "    \"\"\"\n",
    "    def __init__(self, total_timesteps, log_interval=100, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.total_timesteps = total_timesteps\n",
    "        self.log_interval = log_interval\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_count = 0\n",
    "        self.pbar = None\n",
    "        # Track the current episode\n",
    "        self.current_episode_reward = 0\n",
    "        self.current_episode_length = 0\n",
    "        \n",
    "    def _on_training_start(self):\n",
    "        self.pbar = tqdm(total=self.total_timesteps, desc=\"Training\")\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_count = 0\n",
    "        self.current_episode_reward = 0\n",
    "        self.current_episode_length = 0\n",
    "        \n",
    "    def _on_step(self):\n",
    "        # Update progress bar with each step\n",
    "        self.pbar.update(1)\n",
    "        \n",
    "        # Increment episode length counter\n",
    "        self.current_episode_length += 1\n",
    "        \n",
    "        # Add the current reward to the episode total\n",
    "        if 'rewards' in self.locals:\n",
    "            self.current_episode_reward += self.locals['rewards'][0]\n",
    "        \n",
    "        # Track episode rewards and lengths\n",
    "        if self.locals.get('dones')[0]:\n",
    "            self.episode_count += 1\n",
    "            # Store the episode results\n",
    "            self.episode_rewards.append(self.current_episode_reward)\n",
    "            self.episode_lengths.append(self.current_episode_length)\n",
    "            \n",
    "            # Reset counters for the next episode\n",
    "            self.current_episode_reward = 0\n",
    "            self.current_episode_length = 0\n",
    "            \n",
    "            # Log every log_interval episodes\n",
    "            if self.episode_count % self.log_interval == 0:\n",
    "                mean_reward = np.mean(self.episode_rewards[-self.log_interval:])\n",
    "                mean_length = np.mean(self.episode_lengths[-self.log_interval:])\n",
    "                self.pbar.set_postfix({\n",
    "                    'episodes': self.episode_count,\n",
    "                    'mean_reward': f'{mean_reward:.2f}',\n",
    "                    'mean_length': f'{mean_length:.2f}'\n",
    "                })\n",
    "                # Also update the description for better visibility\n",
    "                self.pbar.set_description(\n",
    "                    f\"Training | Episodes: {self.episode_count} | Reward: {mean_reward:.2f}\"\n",
    "                )\n",
    "                \n",
    "        return True\n",
    "        \n",
    "    def _on_training_end(self):\n",
    "        # Close the progress bar\n",
    "        if self.pbar is not None:\n",
    "            self.pbar.close()\n",
    "            self.pbar = None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create environment\n",
    "    env = SARrobotEnv(\n",
    "        grid_rows=4,\n",
    "        grid_cols=4,\n",
    "        info_number_needed=1,\n",
    "        sparse_reward=False,\n",
    "        reward_shaping=False,\n",
    "        attention=False,\n",
    "        hierarchical=False,\n",
    "        render_mode='None'\n",
    "    )\n",
    "    \n",
    "    # Set up total timesteps\n",
    "    total_timesteps = 100000\n",
    "    \n",
    "    # Create the callback\n",
    "    progress_callback = TqdmProgressCallback(\n",
    "        total_timesteps=total_timesteps,\n",
    "        log_interval=100  # Log every 100 episodes\n",
    "    )\n",
    "\n",
    "    # Create the model with custom neural network architecture\n",
    "    policy_kwargs = dict(\n",
    "        net_arch=[64, 64],  # 3 hidden layers with 256, 128, and 64 neurons\n",
    "    )\n",
    "    \n",
    "    # Create and train the agent\n",
    "    model = DQN(\n",
    "        \"MlpPolicy\", \n",
    "        env, \n",
    "        learning_rate=0.001,\n",
    "        buffer_size=2000,\n",
    "        learning_starts=1000,\n",
    "        batch_size=64,\n",
    "        gamma=0.95,\n",
    "        target_update_interval=100,\n",
    "        exploration_initial_eps=1.0,\n",
    "        exploration_fraction=0.3,\n",
    "        exploration_final_eps=0.01,\n",
    "        policy_kwargs=policy_kwargs,  # Add the custom architecture here\n",
    "        verbose=1,\n",
    "        tensorboard_log=\"./dqn_sar_tensorboard/\"\n",
    "    )\n",
    "\n",
    "    # # Define custom network architecture\n",
    "    # policy_kwargs = dict(\n",
    "    #     net_arch=[dict(pi=[64, 64], vf=[64, 64])]  # Separate networks for policy and value\n",
    "    # )\n",
    "\n",
    "    # # Create and train the PPO agent\n",
    "    # model = PPO(\n",
    "    #     \"MlpPolicy\", \n",
    "    #     env,\n",
    "    #     learning_rate=1e-3,\n",
    "    #     n_steps=2048,           # Steps to run for each environment per update\n",
    "    #     batch_size=256,          # Minibatch size for optimization\n",
    "    #     n_epochs=10,            # Number of epochs to optimize for\n",
    "    #     gamma=0.99,             # Discount factor\n",
    "    #     gae_lambda=0.95,        # Factor for trade-off of bias vs variance for GAE\n",
    "    #     clip_range=0.2,         # Clipping parameter for PPO\n",
    "    #     ent_coef=0.1,          # Entropy coefficient for exploration\n",
    "    #     vf_coef=0.5,            # Value function coefficient\n",
    "    #     max_grad_norm=0.5,      # Clipping of gradients\n",
    "    #     policy_kwargs=policy_kwargs,\n",
    "    #     tensorboard_log=\"./dqn_sar_tensorboard/\"\n",
    "    # )\n",
    "    \n",
    "    # Train with the callback\n",
    "    model.learn(\n",
    "        total_timesteps=total_timesteps,\n",
    "        callback=progress_callback\n",
    "    )\n",
    "    \n",
    "    # Save the model\n",
    "    model.save(\"dqn_sar_robot\")\n",
    "    \n",
    "    print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from tqdm.auto import tqdm\n",
    "from environment_sar import SARrobotEnv\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def visualize_evaluation(model, env, n_eval_episodes=10, render=False):\n",
    "    \"\"\"\n",
    "    Evaluate a model and visualize each step\n",
    "    \"\"\"\n",
    "    all_rewards = []\n",
    "    \n",
    "    for episode in range(n_eval_episodes):\n",
    "        print(f\"\\n--- Episode {episode+1}/{n_eval_episodes} ---\")\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Get action from model\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            \n",
    "            # Get action name if available\n",
    "            action_name = f\"Action {action}\"\n",
    "            if hasattr(env, '_get_action_name'):\n",
    "                action_name = env._get_action_name(None, action)\n",
    "            \n",
    "            # Print current step info\n",
    "            print(f\"Step {step+1}: Taking {action_name}\")\n",
    "            \n",
    "            # Render if requested\n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep(0.5)  # Add delay to make rendering visible\n",
    "            \n",
    "            # Take action\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Print result of action\n",
    "            print(f\"  Reward: {reward:.2f}, Total: {total_reward:.2f}\")\n",
    "            \n",
    "            \n",
    "            step += 1\n",
    "        \n",
    "        # Episode summary\n",
    "        print(f\"Episode {episode+1} finished with reward: {total_reward:.2f} in {step} steps\")\n",
    "        all_rewards.append(total_reward)\n",
    "    \n",
    "    # Overall statistics\n",
    "    mean_reward = np.mean(all_rewards)\n",
    "    std_reward = np.std(all_rewards)\n",
    "    print(f\"\\nEvaluation complete: {n_eval_episodes} episodes\")\n",
    "    print(f\"Mean reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n",
    "    \n",
    "    return mean_reward, std_reward\n",
    "\n",
    "# Create the environment (choose render_mode based on what you want)\n",
    "# Use 'human' for visual rendering, 'None' for no rendering\n",
    "eval_env = SARrobotEnv(\n",
    "    grid_rows=4,\n",
    "    grid_cols=4,\n",
    "    info_number_needed=1,\n",
    "    sparse_reward=False,\n",
    "    reward_shaping=False,\n",
    "    attention=False,\n",
    "    hierarchical=False,\n",
    "    render_mode='none'  # Set to 'human' to see visualization\n",
    ")\n",
    "\n",
    "# Load the trained model\n",
    "model = DQN.load(\"dqn_sar_robot\")\n",
    "\n",
    "# Run evaluation with visualization\n",
    "mean_reward, std_reward = visualize_evaluation(\n",
    "    model,\n",
    "    eval_env,\n",
    "    n_eval_episodes=1,  # Start with fewer episodes to see details\n",
    "    render=True  # Set to True to see visual rendering\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
